{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "qNyjMARx4jtH"
   },
   "outputs": [],
   "source": [
    "#Imports for model\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "#Install import_ipynb to import other notebooks\n",
    "# !pip install import_ipynb\n",
    "import import_ipynb\n",
    "import tensorflow as tf\n",
    "# physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BIclN-yggl6",
    "outputId": "e956048f-12bb-4495-fafe-8f5e3d96c9bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4KbG33nrjhAZ"
   },
   "outputs": [],
   "source": [
    "#Import Glove class\n",
    "from glove import Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9C_hrDmYF6Op",
    "outputId": "90be82f5-51a5-46c4-d4d5-490a007d3155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Create instance of glove\n",
    "glove = Glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "gpimHx-DF6Oq",
    "outputId": "16d52b39-9c64-4fb2-c035-69705e230c2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vector(\"__OOV__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "cM9P8LUUF6Oq"
   },
   "outputs": [],
   "source": [
    "#Clean sentence (remove non alpha chars)\n",
    "p = re.compile(r'<.*?>')\n",
    "def cleanSentence(sentence, html = False):\n",
    "    if html: sentence = p.sub('', sentence) \n",
    "    sentence = ''.join([i.lower() if i.isalpha() else \" \" if (i==\" \" or i==\"-\" or i==\"_\") else \"\" for i in sentence])\n",
    "    return sentence\n",
    "\n",
    "#Iterator for given file\n",
    "def iterate_file(file_path):\n",
    "    with open(file_path, \"r\", encoding = 'utf8') as f:\n",
    "        reader = csv.DictReader(f, delimiter=\",\")\n",
    "        for row in reader:\n",
    "            yield row\n",
    "\n",
    "def convert_strlist_to_vec(strlist, max_sequence_length, vec_dim):\n",
    "    vec = np.zeros((max_sequence_length, vec_dim))\n",
    "    for i in range(max_sequence_length):\n",
    "        if i == len(strlist):\n",
    "            break\n",
    "        vec[i] = glove.vector(strlist[i])\n",
    "    return vec\n",
    "\n",
    "# def data_generator(file_path, minibatch_size=5000, max_sequence_length=15, vec_dim=300, Infinite_loop=True):\n",
    "#     i = 0\n",
    "#     X = []\n",
    "#     Y = []\n",
    "#     while True:\n",
    "#         for row in iterate_file(file_path):\n",
    "#             info = row[\"tags\"].split(\"|\") + row[\"title\"].split()\n",
    "#             xi = convert_strlist_to_vec(info, max_sequence_length, vec_dim)\n",
    "#             yi =  (float(row[\"stars\"])>3)\n",
    "#             X.append(xi)\n",
    "#             Y.append(yi)\n",
    "#             i +=1\n",
    "#             if i>=minibatch_size:\n",
    "#                 yield np.array(X), np.array(Y)#, [None]\n",
    "#                 i = 0\n",
    "#                 X = []\n",
    "#                 Y = []\n",
    "#         if not Infinite_loop: break\n",
    "\n",
    "def data_generator(file_path, minibatch_size=5000, max_sequence_length=15, vec_dim=300, Infinite_loop=True):\n",
    "    i = 0\n",
    "    X1 = []\n",
    "    X2= []\n",
    "    Y = []\n",
    "    while True:\n",
    "        for row in iterate_file(file_path):\n",
    "            #----------------\n",
    "            #info = row[\"tags\"].split(\"|\") + [\".\"] +row[\"title\"].split() + [\".\"] #+ row[\"body\"].split()\n",
    "            info = row[\"title\"].split()\n",
    "            x1i = convert_strlist_to_vec(info, max_sequence_length, vec_dim)\n",
    "            #----------------\n",
    "            date = row[\"creation_date\"]\n",
    "            x2i = [0]*10\n",
    "            x2i[int(date[3])] = 1\n",
    "#             print(date, x2i)\n",
    "            #----------------\n",
    "            yi =  float(float(row[\"stars\"])>3)\n",
    "            #----------------\n",
    "            X1.append(x1i)\n",
    "            X2.append(x2i)\n",
    "            Y.append(yi)\n",
    "            i +=1\n",
    "            if i>=minibatch_size:\n",
    "                yield [np.array(X1), np.array(X2)], np.array(Y)#, [None]\n",
    "                i = 0\n",
    "                X1 = []\n",
    "                X2 = []\n",
    "                Y = []\n",
    "        if not Infinite_loop: break\n",
    "\n",
    "# def make_gen_callable(_gen):\n",
    "#         def gen():\n",
    "#             for x,y in _gen:\n",
    "#                  yield x,y\n",
    "#         return gen\n",
    "        \n",
    "# def create_dataset(file_path, minibatch_size=3000, max_sequence_length=15):\n",
    "#     generator = make_gen_callable(data_generator(file_path, minibatch_size, max_sequence_length))\n",
    "#     dataset = tf.data.Dataset.from_generator(generator=generator,\n",
    "#                                              output_types=(tf.float32,tf.bool),\n",
    "#                                              output_shapes = ((None,max_sequence_length,300), (None,))\n",
    "#                                             )\n",
    "#     return dataset\n",
    "        \n",
    "# #Parameters\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "VEC_DIM = 300\n",
    "INPUT_FILE_TRAIN = \"../processed_files/data_stack_shuffled_heavy_train.csv\"\n",
    "INPUT_FILE_VAL = \"../processed_files/data_stack_shuffled_heavy_val.csv\"\n",
    "INPUT_FILE_TEST = \"../processed_files/data_stackoverflow_heavy_test.csv\"\n",
    "\n",
    "train_dataset_size = 604695  #<<<---------------OJO!! xoxo\n",
    "val_dataset_size = 18051\n",
    "\n",
    "minibatch_size = 512\n",
    "train_generator = data_generator(INPUT_FILE_TRAIN, minibatch_size, MAX_SEQUENCE_LENGTH)\n",
    "val_generator = data_generator(INPUT_FILE_VAL, minibatch_size, MAX_SEQUENCE_LENGTH)\n",
    "# train_generator = create_dataset(INPUT_FILE_TRAIN, minibatch_size, MAX_SEQUENCE_LENGTH)\n",
    "# val_generator = create_dataset(INPUT_FILE_VAL, minibatch_size, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "# cont = 0\n",
    "# for data in data_generator(INPUT_FILE_VAL):\n",
    "#     data\n",
    "#     cont +=1\n",
    "#     if cont >= 10:\n",
    "#         break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "7mGgdlYv0OSp",
    "outputId": "42411d36-277a-44fb-ab68-3409ff41df55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"functional_39\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_39 (InputLayer)           [(None, 30, 300)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_37 (Bidirectional (None, 30, 512)      1140736     input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_38 (Bidirectional (None, 512)          1574912     bidirectional_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 512)          0           bidirectional_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_40 (InputLayer)           [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 522)          0           flatten_19[0][0]                 \n",
      "                                                                 input_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 256)          133888      concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 128)          32896       dense_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_59 (Dense)                (None, 1)            129         dense_58[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,882,561\n",
      "Trainable params: 2,882,561\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model #1\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import Input, LSTM, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout, SpatialDropout1D, Concatenate\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# inputs: A 3D tensor with shape [batch, timesteps, feature].\n",
    "# inputs = tf.random.normal([32, 10, 8])\n",
    "\n",
    "# X = SpatialDropout1D(0.4)(Input)\n",
    "# x = LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "#----------------------------------------------------------------------\n",
    "input1 = Input(shape=(MAX_SEQUENCE_LENGTH,300))\n",
    "input2 = Input(shape= (10,))\n",
    "x = Bidirectional(LSTM(256, activation=\"relu\", return_sequences=True))(input1)\n",
    "x = Bidirectional(LSTM(256, activation=\"relu\"))(x)\n",
    "x = Flatten()(x)\n",
    "x = Concatenate()([x, input2])\n",
    "x = Dense(256 ,activation=\"relu\")(x)\n",
    "x = Dense(128 ,activation=\"relu\")(x)\n",
    "# preds = Dense(1)(x)\n",
    "preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model1 = Model([input1, input2], preds)\n",
    "model1.compile(loss = keras.losses.binary_crossentropy,\n",
    "              # loss='mean_squared_error',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.0003),# 0.0003\n",
    "              # metrics=[\"mean_absolute_error\"])\n",
    "              metrics=[\"accuracy\"])\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "5-hAMtNFje3E",
    "outputId": "be616f5e-a475-46ef-9a19-bd920b4e9acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1181/1181 [==============================] - 779s 659ms/step - loss: 0.6659 - accuracy: 0.5947 - val_loss: 0.6638 - val_accuracy: 0.6008\n",
      "Epoch 2/20\n",
      "1181/1181 [==============================] - 785s 664ms/step - loss: 0.6596 - accuracy: 0.6057 - val_loss: 0.6618 - val_accuracy: 0.6049\n",
      "Epoch 3/20\n",
      "1181/1181 [==============================] - 809s 685ms/step - loss: 0.6573 - accuracy: 0.6091 - val_loss: 0.6609 - val_accuracy: 0.6054\n",
      "Epoch 4/20\n",
      "1181/1181 [==============================] - 481s 408ms/step - loss: 0.6565 - accuracy: 0.6099 - val_loss: 0.6615 - val_accuracy: 0.6056\n",
      "Epoch 5/20\n",
      "1181/1181 [==============================] - 479s 406ms/step - loss: 0.6548 - accuracy: 0.6129 - val_loss: 0.6606 - val_accuracy: 0.6074\n",
      "Epoch 6/20\n",
      "1181/1181 [==============================] - 474s 402ms/step - loss: 0.6546 - accuracy: 0.6133 - val_loss: 0.6583 - val_accuracy: 0.6096\n",
      "Epoch 7/20\n",
      "1181/1181 [==============================] - 478s 405ms/step - loss: 0.6533 - accuracy: 0.6150 - val_loss: 0.6575 - val_accuracy: 0.6103\n",
      "Epoch 8/20\n",
      "1181/1181 [==============================] - 479s 406ms/step - loss: 0.6520 - accuracy: 0.6169 - val_loss: 0.6575 - val_accuracy: 0.6120\n",
      "Epoch 9/20\n",
      "1181/1181 [==============================] - 471s 399ms/step - loss: 0.6524 - accuracy: 0.6164 - val_loss: 0.6569 - val_accuracy: 0.6103\n",
      "Epoch 10/20\n",
      "1181/1181 [==============================] - 463s 392ms/step - loss: 0.6520 - accuracy: 0.6170 - val_loss: 0.6561 - val_accuracy: 0.6127\n",
      "Epoch 11/20\n",
      "1181/1181 [==============================] - 461s 390ms/step - loss: 0.6507 - accuracy: 0.6182 - val_loss: 0.6565 - val_accuracy: 0.6100\n",
      "Epoch 12/20\n",
      "1181/1181 [==============================] - 453s 383ms/step - loss: 0.6501 - accuracy: 0.6202 - val_loss: 0.6567 - val_accuracy: 0.6137\n",
      "Epoch 13/20\n",
      "1181/1181 [==============================] - 448s 379ms/step - loss: 0.6496 - accuracy: 0.6203 - val_loss: 0.6565 - val_accuracy: 0.6113\n",
      "Epoch 14/20\n",
      "1181/1181 [==============================] - 445s 377ms/step - loss: 0.6492 - accuracy: 0.6206 - val_loss: 0.6556 - val_accuracy: 0.6134\n",
      "Epoch 15/20\n",
      "1181/1181 [==============================] - 452s 383ms/step - loss: 0.6484 - accuracy: 0.6221 - val_loss: 0.6556 - val_accuracy: 0.6150\n",
      "Epoch 16/20\n",
      "1181/1181 [==============================] - 454s 384ms/step - loss: 0.6484 - accuracy: 0.6217 - val_loss: 0.6555 - val_accuracy: 0.6109\n",
      "Epoch 17/20\n",
      "1181/1181 [==============================] - 456s 386ms/step - loss: 0.6475 - accuracy: 0.6225 - val_loss: 0.6558 - val_accuracy: 0.6146\n",
      "Epoch 18/20\n",
      "1181/1181 [==============================] - 461s 391ms/step - loss: 0.6465 - accuracy: 0.6240 - val_loss: 0.6547 - val_accuracy: 0.6150\n",
      "Epoch 19/20\n",
      "1181/1181 [==============================] - 453s 383ms/step - loss: 0.6472 - accuracy: 0.6234 - val_loss: 0.6561 - val_accuracy: 0.6107\n",
      "Epoch 20/20\n",
      "1181/1181 [==============================] - 450s 381ms/step - loss: 0.6469 - accuracy: 0.6240 - val_loss: 0.6547 - val_accuracy: 0.6156\n"
     ]
    }
   ],
   "source": [
    "#Fit model\n",
    "# model1.fit(input_sequences_train, labels_train, validation_data=(input_sequences_test, labels_test), epochs=50, batch_size=3000)\n",
    "history = model1.fit( \n",
    "            train_generator,\n",
    "            validation_data = val_generator,\n",
    "            steps_per_epoch = train_dataset_size//minibatch_size,\n",
    "            validation_steps = val_dataset_size//minibatch_size,\n",
    "            epochs=20,\n",
    "            use_multiprocessing=False)\n",
    "# model1.fit_generator(train_dataset, validation_data=test_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "ZeBDT9dkmN37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "[[0.33768833]]\n",
      "(1, 10)\n",
      "[[0.4795051]]\n",
      "(1, 10)\n",
      "[[0.38035446]]\n",
      "(1, 10)\n",
      "[[0.41057754]]\n",
      "(1, 10)\n",
      "[[0.5227095]]\n",
      "(1, 10)\n",
      "[[0.2439075]]\n",
      "(1, 10)\n",
      "[[0.5483649]]\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "              \"compressing / decompressing folders & files\",\n",
    "              \"HOW TO decompress and compress files and folders\",\n",
    "              \"how to load a specific version of an assembly\",\n",
    "              \"how would one code test and set behavior without a special hardware instruction?\",\n",
    "             \"can you debug a .net app with only the source code of one file?\",\n",
    "             \"what columns generally make good indexes?\",\n",
    "             \"why is there no generic synchronized queue in .net?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    input_sample = convert_strlist_to_vec(question.split(), MAX_SEQUENCE_LENGTH, VEC_DIM)\n",
    "    input_sample = input_sample[np.newaxis,...]\n",
    "    year = np.array([[0,0,0,0,0,0,0,0,0,1]])\n",
    "    print(year.shape)\n",
    "    print(model1.predict([input_sample, year]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMInEdSQ_8xB",
    "outputId": "c66a0916-14a9-4f45-b2b4-2d215c31a02b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_sequences_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-4e310b5194d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sequences_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m  \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_sequences_test' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = model1.predict(input_sequences_test)\n",
    "for y_hat,y  in zip(predictions[:50], labels_test[:50]):\n",
    "    print(y,\":\", y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtuCYGZBdu8t"
   },
   "outputs": [],
   "source": [
    "#create word to index dictionary: word->index\n",
    "# index 0 is for unnexistent word\n",
    "word_index = {}\n",
    "cont = 0\n",
    "for word in glove.embeddings.keys():\n",
    "    cont +=1\n",
    "    word_index[word] = cont\n",
    "\n",
    "#Processing of GloVe data into the embedding matrix to use in Keras\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, glove.dim))\n",
    "EMBEDDING_DIM = glove.dim\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_matrix[i] = glove.vector(word)\n",
    "\n",
    "# delete glove dictionary to save memory RAM\n",
    "#del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zS7JR6Rix9UU"
   },
   "outputs": [],
   "source": [
    "# Create model #1\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "#del embedding_matrix\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(16, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(16, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(16, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(1, activation='relu')(x)\n",
    "\n",
    "model1 = Model(sequence_input, preds)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "# model1.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "#           epochs=2, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "Y9UrFm5xyGBz"
   },
   "outputs": [],
   "source": [
    "#Export Glove Model\n",
    "import pickle\n",
    "with open(\"../processed_files/glove_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(glove, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export NN model\n",
    "model1.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model_4v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
