{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCF1np0iSl1f"
   },
   "source": [
    "# Import libraries and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qNyjMARx4jtH"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b6d56f25b15f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# pip install alibi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "#Imports for model\n",
    "from __future__ import print_function\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "#Install import_ipynb to import other notebooks\n",
    "# !pip install import_ipynb\n",
    "import import_ipynb\n",
    "import tensorflow as tf\n",
    "# physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# pip install alibi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BIclN-yggl6",
    "outputId": "e956048f-12bb-4495-fafe-8f5e3d96c9bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hdsbd4yXSl1h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "4KbG33nrjhAZ",
    "outputId": "70346fcb-c60e-440e-c75e-c235db9769e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 128)\n",
      "<class 'numpy.ndarray'>\n",
      "[[ 2.71079719e-01 -1.05507271e-02 -5.72839715e-02  6.85367882e-02\n",
      "  -8.43827128e-02  2.23962113e-01 -2.47001299e-03 -9.79759842e-02\n",
      "  -6.09251820e-02  1.67842228e-02  1.83305964e-02 -2.68354714e-02\n",
      "   1.98764652e-02  2.20524482e-02  3.80336978e-02  2.34529171e-02\n",
      "  -5.35214022e-02 -2.91685425e-02 -1.38161421e-01  2.55649000e-01\n",
      "   5.48296236e-03  8.99440721e-02  9.70285609e-02 -1.61739327e-02\n",
      "   1.52733102e-01  3.44900675e-02  5.59903085e-02  1.96482558e-02\n",
      "  -1.90152451e-02  1.16014794e-01  6.57583252e-02 -3.56089808e-02\n",
      "  -2.41284538e-02 -7.16865994e-03 -8.95059258e-02 -1.02139087e-02\n",
      "   7.43148699e-02 -1.04629382e-01 -3.95198241e-02  2.72065704e-03\n",
      "  -1.46868704e-02 -1.35065345e-02 -4.82564233e-02  3.08891684e-02\n",
      "  -4.48269024e-02 -1.74376536e-02  1.03488296e-01  4.14922759e-02\n",
      "  -3.97918411e-02  3.87827717e-02  1.52737334e-01 -9.22826156e-02\n",
      "  -1.72395855e-02  1.83061399e-02 -2.07548272e-02  8.00881982e-02\n",
      "  -8.07149112e-02 -1.55732170e-01  1.38933867e-01  6.14028722e-02\n",
      "  -5.63981198e-02 -5.52625693e-02 -2.76599266e-02 -1.75832003e-01\n",
      "   1.03400731e-02 -1.95506945e-01  6.20926507e-02 -3.19367759e-02\n",
      "   8.83726701e-02 -5.62930889e-02  9.26868394e-02  5.39597832e-02\n",
      "  -1.90052669e-02 -1.76269934e-01 -3.10309380e-02 -1.22043781e-01\n",
      "  -1.57093350e-03 -7.85367787e-02 -5.89226782e-02  1.13113113e-01\n",
      "   7.49601750e-03  1.77206352e-01  7.05702975e-03 -2.47793645e-02\n",
      "   1.75480787e-02 -1.07081890e-01  1.35404365e-02 -2.27676779e-02\n",
      "   1.84805185e-01  2.38784611e-01  2.69084163e-02  1.74863771e-01\n",
      "   1.54453337e-01  4.33688052e-02  1.00303210e-01  3.47379111e-02\n",
      "   1.36952773e-01 -1.13518601e-02  1.03397824e-01 -4.21671383e-02\n",
      "   1.54141873e-01  8.16382747e-03 -9.01643559e-02  5.38221449e-02\n",
      "  -4.68258327e-03 -3.37895863e-02 -6.59089759e-02  3.23954374e-02\n",
      "  -5.41664846e-02  5.77812875e-03  3.59992380e-03 -1.19778901e-01\n",
      "   3.96030098e-02 -9.63303745e-02 -3.63231674e-02  4.46534716e-02\n",
      "   8.64548534e-02  3.43542211e-02 -1.63717885e-02 -2.09665615e-02\n",
      "   1.08085752e-01 -9.29963440e-02  6.65557161e-02 -4.05292846e-02\n",
      "  -8.08768794e-02 -1.57664806e-01  3.12358551e-02  2.21031159e-02]\n",
      " [-2.39143893e-01  1.58596992e-01 -8.62944126e-02  8.68592039e-02\n",
      "   1.81387275e-01 -7.29715154e-02 -1.36163428e-01  1.49282843e-01\n",
      "  -3.68973799e-02  1.00761289e-02  2.06087297e-03  1.92960933e-01\n",
      "  -4.57838438e-02 -5.94148263e-02 -2.75511369e-02 -5.46233132e-02\n",
      "  -8.18079486e-02 -6.95111081e-02 -2.31150334e-04 -4.93146479e-02\n",
      "  -9.69947800e-02  6.83198869e-02  3.28927785e-02 -2.10557610e-01\n",
      "  -1.88999120e-02  7.80548230e-02  4.39702086e-02 -1.01595456e-02\n",
      "   5.90811856e-02  3.79904322e-02  5.12019694e-02  1.42406207e-04\n",
      "  -1.00271136e-01  1.21258646e-01  8.98982119e-03 -3.93984579e-02\n",
      "   5.84119558e-02 -7.29160160e-02 -2.04813957e-01  1.00768372e-01\n",
      "   4.67725396e-02 -7.97163397e-02  1.19984746e-01  1.40121907e-01\n",
      "   9.05964226e-02 -4.40793000e-02  4.58641797e-02 -9.51933935e-02\n",
      "  -2.14765847e-01 -1.86369658e-01 -8.25171918e-02  5.18620424e-02\n",
      "  -1.45826399e-01  1.84234530e-02  4.74787578e-02 -5.66627868e-02\n",
      "  -1.90120950e-01  1.99522153e-01 -9.70014781e-02 -8.36818665e-02\n",
      "  -2.20062630e-03 -1.77345678e-01  1.26274943e-01  9.42042917e-02\n",
      "   5.02898619e-02  1.12796091e-01  5.39428480e-02  3.77548784e-02\n",
      "   7.43291825e-02 -8.32043774e-03 -3.70244905e-02 -9.45623890e-02\n",
      "   1.46631882e-01  1.64107248e-01  1.50251880e-01 -5.75655662e-02\n",
      "   1.64375588e-01  2.23196074e-02 -1.45043299e-01  3.79186636e-03\n",
      "   9.08595789e-03  1.50048912e-01 -3.65889166e-03 -4.54103015e-02\n",
      "   1.13636285e-01 -2.61905175e-02  7.94217363e-02  1.46186411e-01\n",
      "  -9.59541872e-02 -1.00506961e-01  9.53687578e-02  7.44143948e-02\n",
      "  -5.21411635e-02  7.83440173e-02 -3.52163352e-02 -2.10600365e-02\n",
      "   1.15768299e-01  4.81865741e-02  4.55700159e-02  1.85692072e-01\n",
      "   5.07994555e-02  1.26084119e-01 -1.68005273e-01 -7.05936179e-03\n",
      "   5.92864379e-02 -7.05262199e-02 -2.48226412e-02  7.84422681e-02\n",
      "  -1.67815182e-02  3.85983139e-02 -4.17775009e-03  2.36217938e-02\n",
      "   3.44562624e-03  1.23821497e-02 -6.60593137e-02  2.93071195e-03\n",
      "  -1.02906160e-01  1.35316402e-01 -5.14262840e-02  8.51522535e-02\n",
      "   6.48412434e-03  1.76049583e-02  3.72802950e-02  6.58616498e-02\n",
      "   1.17918454e-01  1.60586953e-01  4.60696220e-02  6.83866367e-02]]\n"
     ]
    }
   ],
   "source": [
    "# embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n",
    "embed = hub.load(\"models\\_nnlm_en_dim128\")\n",
    "embeddings = embed([\"cat is on the mat\", \" \"])\n",
    "print(embeddings.shape)\n",
    "print(type(embeddings.numpy()))\n",
    "print(embeddings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4vdurKESl1h"
   },
   "outputs": [],
   "source": [
    "# %timeit batch1 = embed([\"cat\"]*15*5000)\n",
    "# batch1 = embed([\"cat\"]*15*5000)\n",
    "# %timeit batch2 = np.array([embed([\"cat\"]*15) for i in range(5000)])\n",
    "# batch2 = np.array([embed([\"cat\"]*15) for i in range(5000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XuzQx-igSl1h"
   },
   "outputs": [],
   "source": [
    "# b1 = batch1.numpy().copy()\n",
    "# b2 = batch2.copy()\n",
    "# print(\"shape b1:\",b1.shape)\n",
    "# print(\"shape b2:\",b2.shape)\n",
    "# b3 = b1.reshape(5000,15, -1)\n",
    "# print(\"reshape b1:\",b3.shape)\n",
    "# print(b2[100,2,30], b3[100,2,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3zENvXVSl1h"
   },
   "outputs": [],
   "source": [
    "# glove.vector(\"?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hBZuWEDSl1h"
   },
   "source": [
    "# Input Pipeline: Data converters and Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1oXX7sl6Sl1i"
   },
   "outputs": [],
   "source": [
    "#Clean sentence (remove non alpha chars)\n",
    "p = re.compile(r'<.*?>')\n",
    "def cleanSentence(sentence, html = False):\n",
    "    if html: sentence = p.sub('', sentence)\n",
    "    sentence = ''.join([i.lower() if i.isalpha() else \" \" if (i==\" \" or i==\"-\" or i==\"_\") else \"\" for i in sentence])\n",
    "    return sentence\n",
    "\n",
    "#Iterator for given file\n",
    "def iterate_file(file_path):\n",
    "    with open(file_path, \"r\", encoding = 'utf8') as f:\n",
    "        reader = csv.DictReader(f, delimiter=\",\")\n",
    "        for row in reader:\n",
    "            yield row\n",
    "\n",
    "def convert_strlist_to_vec(strlist, max_sequence_length, vec_dim):\n",
    "    vec = np.zeros((max_sequence_length, vec_dim))\n",
    "    for i in range(max_sequence_length):\n",
    "        if i == len(strlist):\n",
    "            break\n",
    "        vec[i] = glove.vector(strlist[i])\n",
    "    return vec\n",
    "\n",
    "def data_generator(file_path, minibatch_size=5000, max_sequence_length=15, vec_dim=300, return_text=False, Infinite_loop=True):\n",
    "    i = 0\n",
    "    Xtext = []\n",
    "    X2= []\n",
    "    Mask = [] #mask to multiply text embeddings and set to 0 padding vectors \n",
    "    Y = []\n",
    "    while True:\n",
    "        for row in iterate_file(file_path):\n",
    "            #----------------\n",
    "#             info = [row[\"body\"]] + row[\"tags\"].split(\"|\") + [\".\"] +row[\"title\"].split() #+ [\".\"] + row[\"body\"].split()\n",
    "            info = row[\"tags\"].split(\"|\") + [\".\"] +row[\"title\"].split() #+ [\".\"] + row[\"body\"].split()\n",
    "\n",
    "            # set x1i length equal to str of max_sequence_lenght\n",
    "            xtexti =  info[0:max_sequence_length]  +  [\"\"]*(max_sequence_length-len(info))\n",
    "            maski = [1]*min(len(info), max_sequence_length) + [0]*(max_sequence_length-len(info))\n",
    "            #----------------\n",
    "            date = row[\"creation_date\"]\n",
    "            year = int(date[3])\n",
    "            \n",
    "            x2i = [0]*10\n",
    "            x2i[year] = 1\n",
    "#             print(date, x2i)\n",
    "            #----------------\n",
    "#             hour = int(date[11:13])\n",
    "#             print(date, hour)\n",
    "#             x3i = [0]*24\n",
    "#             x3i[hour] = 1\n",
    "\n",
    "#             print(date, x2i)\n",
    "            #----------------\n",
    "#             yi =  (float(row[\"stars\"])>3)\n",
    "            yi = float(row[\"score\"])>2 #<----------------OJO!! (Naive evaluator) stack: median>0, math: median>2\n",
    "            #----------------\n",
    "            Xtext += xtexti #<--------------------------OJO!!!  (sum all batch words into single array to speedup emmbeding)  \n",
    "            Mask.append(maski)\n",
    "            X2.append(x2i)\n",
    "            Y.append(yi)\n",
    "            i +=1\n",
    "            if i>=minibatch_size:\n",
    "                X1 = np.array(embed(Xtext))\n",
    "                \n",
    "                Mask = np.array(Mask)\n",
    "                Mask = np.expand_dims(Mask, axis=-1)\n",
    "#                 print(Mask[0,:])\n",
    "                X1 = Mask * X1.reshape(minibatch_size, max_sequence_length, vec_dim)\n",
    "#                 print(X1[0,:,:])\n",
    "                \n",
    "                X2 = np.array(X2)\n",
    "                X2 = X2.reshape(minibatch_size, 1, 10)\n",
    "                O = np.ones((minibatch_size, max_sequence_length, 10)) #for broadcasting year one-hot encoding\n",
    "                X2 = X2 * O\n",
    "                \n",
    "                if return_text:\n",
    "                    Xtext = np.array(Xtext).reshape(minibatch_size, max_sequence_length)\n",
    "                    yield np.concatenate((X2, X1), axis=2), np.array(Y), Xtext\n",
    "#                 yield [tf.convert_to_tensor(X1, tf.float32), tf.convert_to_tensor(X2, tf.float32)], tf.convert_to_tensor(Y, tf.float32)\n",
    "                else:\n",
    "                    yield np.concatenate((X2, X1), axis=2), np.array(Y)\n",
    "#                 yield [tf.convert_to_tensor(X1, tf.float32), tf.convert_to_tensor(X2, tf.float32)], tf.convert_to_tensor(Y, tf.float32)\n",
    "                i = 0\n",
    "                Xtext = []\n",
    "                X2 = []\n",
    "                Mask = []\n",
    "                Y = []\n",
    "        if not Infinite_loop: break\n",
    "        \n",
    "# #Parameters\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "VEC_DIM = 128\n",
    "# INPUT_FILE_TRAIN = \"data_stackoverflow_lightshuffled_train.csv\"\n",
    "INPUT_FILE_TRAIN = \"data_mathematica\\data_mathematica_train.csv\"\n",
    "INPUT_FILE_VAL = \"data_mathematica\\data_mathematica_val.csv\"\n",
    "INPUT_FILE_TEST = \"data_mathematica\\data_mathematica_test.csv\"\n",
    "\n",
    "train_dataset_size = 58788  #<<<---------------OJO!! xoxo\n",
    "val_dataset_size = 3266\n",
    "test_dataset_size = 3266\n",
    "\n",
    "minibatch_size = 1024\n",
    "train_generator = data_generator(INPUT_FILE_TRAIN, minibatch_size, MAX_SEQUENCE_LENGTH, VEC_DIM)\n",
    "val_generator = data_generator(INPUT_FILE_VAL, minibatch_size, MAX_SEQUENCE_LENGTH, VEC_DIM)\n",
    "test_generator = data_generator(INPUT_FILE_TEST, minibatch_size, MAX_SEQUENCE_LENGTH, VEC_DIM)\n",
    "# train_generator = create_dataset(INPUT_FILE_TRAIN, minibatch_size, MAX_SEQUENCE_LENGTH)\n",
    "# val_generator = create_dataset(INPUT_FILE_VAL, minibatch_size, MAX_SEQUENCE_LENGTH)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5PzgX36Sl1i"
   },
   "outputs": [],
   "source": [
    "# for data in val_generator:\n",
    "#     x = data\n",
    "#     break\n",
    "# print(x[0].shape)\n",
    "# print(x[0][0:2,:,0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jz9iPTnbSl1i"
   },
   "source": [
    "# Model: Creation, training, evaluation and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mGgdlYv0OSp",
    "outputId": "682dfd72-0dbe-4144-d974-dd2051599476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 20, 138)]         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 20, 192)           180480    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 96)                110976    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               12416     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 304,001\n",
      "Trainable params: 304,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model #1\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import Input, LSTM, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout, SpatialDropout1D, Concatenate, AveragePooling1D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# inputs: A 3D tensor with shape [batch, timesteps, feature].\n",
    "# inputs = tf.random.normal([32, 10, 8])\n",
    "\n",
    "# X = SpatialDropout1D(0.4)(Input)\n",
    "# x = LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "#----------------------------------------------------------------------\n",
    "input1 = Input(shape=(MAX_SEQUENCE_LENGTH,128+10))\n",
    "# input2 = Input(shape= (10,))\n",
    "x = Bidirectional(LSTM(96, activation=\"relu\", return_sequences=True))(input1)\n",
    "x = LSTM(96, activation=\"relu\")(x)\n",
    "#x = AveragePooling1D(pool_size=MAX_SEQUENCE_LENGTH)(x)\n",
    "x = Flatten()(x)\n",
    "# x = Concatenate()([x, input2])\n",
    "# x = Dense(512 ,activation=\"relu\")(x)\n",
    "x = Dense(128 ,activation=\"relu\")(x)\n",
    "# preds = Dense(1)(x)\n",
    "preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model1 = Model(input1, preds)\n",
    "model1.compile(loss = keras.losses.binary_crossentropy,\n",
    "              # loss='mean_squared_error',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.00003),# 0.0003\n",
    "              # metrics=[\"mean_absolute_error\"])\n",
    "              metrics=[\"accuracy\"])\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-hAMtNFje3E",
    "outputId": "6b13698a-a45d-4845-a140-4267943e86bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 57 steps, validate for 3 steps\n",
      "Epoch 1/20\n",
      "57/57 [==============================] - 8s 132ms/step - loss: 0.5893 - accuracy: 0.6847 - val_loss: 0.6097 - val_accuracy: 0.6605\n",
      "Epoch 2/20\n",
      "57/57 [==============================] - 8s 133ms/step - loss: 0.5891 - accuracy: 0.6821 - val_loss: 0.6049 - val_accuracy: 0.6628\n",
      "Epoch 3/20\n",
      "57/57 [==============================] - 8s 132ms/step - loss: 0.5888 - accuracy: 0.6834 - val_loss: 0.6092 - val_accuracy: 0.6608\n",
      "Epoch 4/20\n",
      "57/57 [==============================] - 8s 135ms/step - loss: 0.5884 - accuracy: 0.6851 - val_loss: 0.6063 - val_accuracy: 0.6605\n",
      "Epoch 5/20\n",
      "57/57 [==============================] - 7s 131ms/step - loss: 0.5879 - accuracy: 0.6839 - val_loss: 0.6066 - val_accuracy: 0.6634\n",
      "Epoch 6/20\n",
      "57/57 [==============================] - 8s 133ms/step - loss: 0.5872 - accuracy: 0.6856 - val_loss: 0.6090 - val_accuracy: 0.6624\n",
      "Epoch 7/20\n",
      "57/57 [==============================] - 8s 136ms/step - loss: 0.5873 - accuracy: 0.6852 - val_loss: 0.6069 - val_accuracy: 0.6654\n",
      "Epoch 8/20\n",
      "57/57 [==============================] - 7s 131ms/step - loss: 0.5869 - accuracy: 0.6848 - val_loss: 0.6125 - val_accuracy: 0.6595\n",
      "Epoch 9/20\n",
      "57/57 [==============================] - 8s 132ms/step - loss: 0.5863 - accuracy: 0.6862 - val_loss: 0.6096 - val_accuracy: 0.6647\n",
      "Epoch 10/20\n",
      "57/57 [==============================] - 7s 131ms/step - loss: 0.5864 - accuracy: 0.6861 - val_loss: 0.6136 - val_accuracy: 0.6608\n",
      "Epoch 11/20\n",
      "57/57 [==============================] - 7s 131ms/step - loss: 0.5853 - accuracy: 0.6869 - val_loss: 0.6151 - val_accuracy: 0.6654\n",
      "Epoch 12/20\n",
      "57/57 [==============================] - 8s 132ms/step - loss: 0.5851 - accuracy: 0.6867 - val_loss: 0.6153 - val_accuracy: 0.6598\n",
      "Epoch 13/20\n",
      "57/57 [==============================] - 7s 131ms/step - loss: 0.5850 - accuracy: 0.6869 - val_loss: 0.6135 - val_accuracy: 0.6608\n",
      "Epoch 14/20\n",
      "57/57 [==============================] - 8s 135ms/step - loss: 0.5842 - accuracy: 0.6879 - val_loss: 0.6136 - val_accuracy: 0.6615\n",
      "Epoch 15/20\n",
      "57/57 [==============================] - 8s 133ms/step - loss: 0.5847 - accuracy: 0.6879 - val_loss: 0.6114 - val_accuracy: 0.6585\n",
      "Epoch 16/20\n",
      "57/57 [==============================] - 8s 132ms/step - loss: 0.5838 - accuracy: 0.6881 - val_loss: 0.6089 - val_accuracy: 0.6618\n",
      "Epoch 17/20\n",
      "57/57 [==============================] - 8s 132ms/step - loss: 0.5835 - accuracy: 0.6885 - val_loss: 0.6088 - val_accuracy: 0.6634\n",
      "Epoch 18/20\n",
      "57/57 [==============================] - 7s 130ms/step - loss: 0.5830 - accuracy: 0.6888 - val_loss: 0.6042 - val_accuracy: 0.6644\n",
      "Epoch 19/20\n",
      "57/57 [==============================] - 8s 132ms/step - loss: 0.5829 - accuracy: 0.6888 - val_loss: 0.6081 - val_accuracy: 0.6628\n",
      "Epoch 20/20\n",
      "57/57 [==============================] - 7s 131ms/step - loss: 0.5826 - accuracy: 0.6897 - val_loss: 0.6039 - val_accuracy: 0.6634\n"
     ]
    }
   ],
   "source": [
    "#Fit model\n",
    "# model1.fit(input_sequences_train, labels_train, validation_data=(input_sequences_test, labels_test), epochs=50, batch_size=3000)\n",
    "history = model1.fit( \n",
    "            train_generator,\n",
    "            validation_data = val_generator,\n",
    "            steps_per_epoch = train_dataset_size//minibatch_size,\n",
    "            validation_steps = val_dataset_size//minibatch_size,\n",
    "            epochs=20,\n",
    "            use_multiprocessing=False)\n",
    "# model1.fit_generator(train_dataset, validation_data=test_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lSPUmvCSl1i"
   },
   "outputs": [],
   "source": [
    "print(\"model evaluation:\")\n",
    "print(\"Loss and accuracy:\", model1.evaluate_generator(test_generator, steps=test_dataset_size/minibatch_size))\n",
    "y_hat = model1.predict_generator(test_generator, steps=test_dataset_size/minibatch_size)\n",
    "print(\"percentage of positive predictions:\", sum(y_hat>0.5)/len(y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeBDT9dkmN37"
   },
   "outputs": [],
   "source": [
    "questions = [             \"Garbage. how do I sort an array in assembler\"\n",
    "#               \"compressing / decompressing folders & files\",\n",
    "#               \"HOW TO decompress and compress files and folders\",\n",
    "#               \"how to load a specific version of an assembly\",\n",
    "#               \"how would one code test and set behavior without a special hardware instruction?\",\n",
    "#              \"can you debug a .net app with only the source code of one file?\",\n",
    "#              \"what columns generally make good indexes?\",\n",
    "#              \"why is there no generic synchronized queue in .net?\",\n",
    "\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    input_sample = convert_strlist_to_vec(question, MAX_SEQUENCE_LENGTH, VEC_DIM)\n",
    "    input_sample = input_sample[np.newaxis,...]\n",
    "    inputs = [input_sample, np.array([[0,0,0,0,0,0,0,0,0,1]])]\n",
    "    print(model1.predict(inputs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Y9UrFm5xyGBz",
    "outputId": "6e8768a3-7a79-4837-f936-3742eb36c0f6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-243d771b4622>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"models\\model6_mathematica_128_nobody_0664.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model1' is not defined"
     ]
    }
   ],
   "source": [
    "model1.save(\"models\\model6_mathematica_128_nobody_0664.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ns_qFtLiSl1i",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1 = None\n",
    "embed = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjgeO6jvSl1i"
   },
   "source": [
    "# Explain model with Alibi Deep Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpYPib7sSl1j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "import re\n",
    "from alibi.explainers import IntegratedGradients\n",
    "\n",
    "class Prediction:\n",
    "    def __init__(self, \n",
    "                 path_model=\"models\\model6_mathematica_128_nobody_0664.h5\",\n",
    "                 path_embeddings=\"models\\_nnlm_en_dim128\"):#-------> DOWNLOAD LINK : https://tfhub.dev/google/nnlm-en-dim128/2\n",
    "        \n",
    "        self.model = keras.models.load_model(path_model)\n",
    "        self.embed = hub.load(path_embeddings)\n",
    "        #regular expression for text pre-processing\n",
    "        self.p = re.compile(r'<.*?>')\n",
    "        #----------------------\n",
    "        n_steps = 50\n",
    "        method = \"gausslegendre\"\n",
    "        internal_batch_size = 100\n",
    "        nb_samples = 5\n",
    "        self.ig = IntegratedGradients(self.model,\n",
    "                          layer=None,\n",
    "                          method=\"gausslegendre\",\n",
    "                          n_steps=50,\n",
    "                          internal_batch_size=100)\n",
    "\n",
    "    #special clean, only for mathematica dataset. \n",
    "    def cleanSentence(self, sentence, html = False):\n",
    "        if html: sentence = self.p.sub('', sentence) \n",
    "        sentence = ''.join([i.lower() if (i.isalpha() or i==\" \") else (\" \"+i) for i in sentence])\n",
    "        return sentence\n",
    "\n",
    "    def predict(self, question, tags, year, max_sequence_length=20, vec_dim=128):\n",
    "        #INPUTS: question: \"string\", tags: [\"string\", \"string\", \"string\"], year: integer (2010-2019) \n",
    "        #OUTPUTS: evaluation: Bool,\n",
    "#                 words: [\"string\", \"string\", \"string\"],\n",
    "#                 attributions_words: [float, float, float] \n",
    "#                 attribution year: float\n",
    "        Xtext=[]\n",
    "        X2=[]\n",
    "        Mask=[]\n",
    "        Y=[]\n",
    "        \n",
    "        clean_question = self.cleanSentence(question)\n",
    "        info = tags+[\".\"]+clean_question.split() \n",
    "        \n",
    "        # set x1i length equal to str of max_sequence_lenght\n",
    "        xtexti =  info[0:max_sequence_length]  +  [\"\"]*(max_sequence_length-len(info))\n",
    "        maski = [1]*min(len(info), max_sequence_length) + [0]*(max_sequence_length-len(info))\n",
    "        #----------------\n",
    "        year = int(str(year)[3])\n",
    "        x2i = [0]*10\n",
    "        x2i[year] = 1\n",
    "        \n",
    "        #-----------------------\n",
    "        Xtext += xtexti\n",
    "        Mask.append(maski)\n",
    "        X2.append(x2i)\n",
    "        \n",
    "        X1 = np.array(self.embed(Xtext))\n",
    "        Mask = np.array(Mask)\n",
    "        Mask = np.expand_dims(Mask, axis=-1)\n",
    "        X1 = Mask * X1.reshape(1, max_sequence_length, vec_dim)\n",
    "        \n",
    "        X2 = np.array(X2)\n",
    "        X2 = X2.reshape(1, 1, 10)\n",
    "        O = np.ones((1, max_sequence_length, 10)) #for broadcasting year one-hot encoding\n",
    "        X2 = X2 * O\n",
    "        #-----------------------------------\n",
    "        X = np.concatenate((X2, X1), axis=2)\n",
    "        prediction = self.model.predict(X)\n",
    "        explanation = self.ig.explain(X,\n",
    "                         baselines=None,\n",
    "                         target=prediction)\n",
    "        \n",
    "        attrs = -explanation.attributions\n",
    "        #separate attributions from year information and text information \n",
    "        attrs1 = attrs[:,:,10:]\n",
    "        attrs2 = attrs[:,:,0:10]\n",
    "#         print(\"attributions 1 and 2 shapes:\", attrs1.shape, attrs2.shape)\n",
    "        # sum attribution by embedding vector \n",
    "        attrs1 = attrs1.sum(axis=2)\n",
    "        attrs1 = attrs1.reshape(-1).tolist()\n",
    "        # sum all year attributions\n",
    "        attrs2 = np.sum(attrs2)\n",
    "        \n",
    "        \n",
    "        return prediction[0][0]>0.5, info, attrs1[:len(info)], attrs2\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12jnPH1kSl1j"
   },
   "outputs": [],
   "source": [
    "prediction = Prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GJHadXKSl1j",
    "outputId": "4224644a-ac6f-466a-da01-5579760c6e1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: True\n",
      "options :\t 0.0647229421760869\n",
      "cluster-analysis :\t 0.057587823504428995\n",
      ". :\t -0.00011027021463007845\n",
      "how :\t 0.01093441714085416\n",
      "to :\t 0.0036322602615675554\n",
      "specify :\t 0.017180457822259253\n",
      "radius :\t 0.02028227530893322\n",
      "for :\t 0.016437871516644477\n",
      "\"meanshift :\t 0.0402741246707585\n",
      "\" :\t 0.018634616515585265\n",
      "method :\t 0.013815074772924051\n",
      "of :\t 0.009817239503394031\n",
      "findclusters :\t 0.023840824771732477\n",
      "attribution all words: 0.2970496577505388\n",
      "attribution year: -0.14286600895672696\n"
     ]
    }
   ],
   "source": [
    "y, words, Awords, Ayear = prediction.predict(\"how to specify radius for  \\\"meanshift\\\" method of findclusters\", [\"options\",\"cluster-analysis\"], 2018)\n",
    "\n",
    "print(\"prediction:\", y)\n",
    "\n",
    "for word, attribution in zip(words, Awords):\n",
    "    print(word,\":\\t\", attribution)\n",
    "    \n",
    "print(\"attribution all words:\", sum(Awords))\n",
    "print(\"attribution year:\", Ayear)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zc_KVXMoSl1j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model_6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
