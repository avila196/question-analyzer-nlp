{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA (Exploratory Data Analysis) performed in the Stack Overflow Dataset.\n",
    "\n",
    "Some EDA techniques are performed in the processed CSV files for the Stack Overflow Dataset in order to check if the\n",
    "selected features from dataset are relevant, measuring some useful characteristics related to word encoders, outliers, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lDnmSWkaZSMP"
   },
   "outputs": [],
   "source": [
    "#Imports for analysis\n",
    "from os import path\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9yQRqvCf6S8",
    "outputId": "b10414c8-7bc5-418e-f2cd-e4a28a317592"
   },
   "outputs": [],
   "source": [
    "#Install import_ipynb to import other notebooks\n",
    "!pip install import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and prepare the Pre-trained GloVe Word Embedding model\n",
    "path_to_glove_zipfile = \"../processed_files/glove.42B.300d.zip\"\n",
    "path_to_glove_file = \"../processed_files/glove.42B.300d.txt\"\n",
    "\n",
    "if not path.exists(path_to_glove_file):\n",
    "    if not path.exists(path_to_glove_zipfile):\n",
    "        print(\"downloading glove .zip file...\")\n",
    "        !wget http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
    "    print(\"unzipping glove .zip file...\")\n",
    "    !unzip -q glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XH7deeIxlDZ9",
    "outputId": "2e47882c-8d6f-4784-d929-5f0c1bf098a0"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'glove'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-eabb783f6cee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Import Glove class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mglove\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGlove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'glove'"
     ]
    }
   ],
   "source": [
    "#Import Glove class\n",
    "from glove import Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0nQi2q2jyZT",
    "outputId": "78165ea2-815d-46a3-9d7d-f08247e0e4eb"
   },
   "outputs": [],
   "source": [
    "#Instance of Glove\n",
    "glove = Glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7kojguGTahPv"
   },
   "outputs": [],
   "source": [
    "#Iterator for given file\n",
    "def iterate_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f, delimiter=\",\")\n",
    "        for row in reader:\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u5pvqkbFj8zC",
    "outputId": "2d8c98ca-7899-4522-d6f8-520c9b86caaa"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b21a33ea0879>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterate_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../processed_files/data_stackoverflow_train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mone_unkown\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mcount_questions\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# get positive and negative stars for every word\n",
    "# get percentage of unkown words for vocab\n",
    "\n",
    "count_wordsinvocab = 0\n",
    "count_words = 0\n",
    "count_questions_with_unkown = 0\n",
    "count_questions = 0\n",
    "weights_directory = {}\n",
    "unkown_counter = {} \n",
    "\n",
    "for row in iterate_file(\"../processed_files/data_stackoverflow_train.csv\"):\n",
    "    words = row.split(\" \")\n",
    "    one_unkown=False\n",
    "    count_questions +=1\n",
    "    for word in words:\n",
    "        if word!=\"\":\n",
    "            count_words += 1\n",
    "            if count_words%1000000==0:\n",
    "                print(\"words:\",count_words)\n",
    "            if word in glove.embeddings:\n",
    "                count_wordsinvocab +=1\n",
    "            else:\n",
    "                unkown_counter[word] = unkown_counter.get(word, 0)+1\n",
    "                one_unkown=True\n",
    "        \n",
    "    count_questions_with_unkown += one_unkown\n",
    "    \n",
    "unkown_counts = [count for count in unkown_counter.values()]\n",
    "unkown_counts.sort()\n",
    "\n",
    "for count in unkown_counts[-10:]:\n",
    "    print(count, [word for word,counti in unkown_counter.items() if count==counti])\n",
    "\n",
    "print(\"percentage of words in vocab:\", count_wordsinvocab / count_words)\n",
    "print(\"percentage of question with unkown words:\", count_questions_with_unkown/count_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "CUH7vsqraVsy",
    "outputId": "04fb4ae1-15e5-4ff3-a03c-9a1823ef56b6"
   },
   "outputs": [],
   "source": [
    "#Find relation between dates and score of questions\n",
    "#Create dictionary of years\n",
    "years = {}\n",
    "\n",
    "#Reads input file and updates dates inside the dictionary\n",
    "def update_dates(filename):\n",
    "    #Iterate through file\n",
    "    for row in iterate_file(filename):\n",
    "        #Get year data\n",
    "        year = int(row[\"creation_date\"].split(\"T\")[0].split(\"-\")[0])\n",
    "        #Append year into dictionary with its score\n",
    "        if year in years:\n",
    "            years[year].append(int(row[\"score\"]))\n",
    "        else:\n",
    "            years[year] = [(int(row[\"score\"]))]\n",
    "\n",
    "#Read all 3 files to fill up years dictionary\n",
    "update_dates(\"../processed_files/data_stackoverflow_test.csv\")\n",
    "update_dates(\"../processed_files/data_stackoverflow_train.csv\")\n",
    "update_dates(\"../processed_files/data_stackoverflow_val.csv\")\n",
    "\n",
    "means = [np.mean(scores) for year, scores in years.items()]\n",
    "\n",
    "plt.plot(list(years.keys()), means)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Mean Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
