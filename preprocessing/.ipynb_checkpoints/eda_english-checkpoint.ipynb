{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA (Exploratory Data Analysis) performed in the English Dataset.\n",
    "\n",
    "Some EDA techniques are performed in the processed CSV files for the English Dataset in order to check if the\n",
    "selected features from dataset are relevant, measuring some useful characteristics related to word encoders, outliers, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDnmSWkaZSMP"
   },
   "outputs": [],
   "source": [
    "#Imports for analysis\n",
    "from os import path\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9yQRqvCf6S8",
    "outputId": "eaad1b67-43a9-4dca-bdf9-d4e2344ddf95"
   },
   "outputs": [],
   "source": [
    "#Install import_ipynb to import other notebooks\n",
    "!pip install import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and prepare the Pre-trained GloVe Word Embedding model\n",
    "path_to_glove_zipfile = \"../processed_files/glove.42B.300d.zip\"\n",
    "path_to_glove_file = \"../processed_files/glove.42B.300d.txt\"\n",
    "\n",
    "if not path.exists(path_to_glove_file):\n",
    "    if not path.exists(path_to_glove_zipfile):\n",
    "        print(\"downloading glove .zip file...\")\n",
    "        !wget http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
    "    print(\"unzipping glove .zip file...\")\n",
    "    !unzip -q glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XH7deeIxlDZ9"
   },
   "outputs": [],
   "source": [
    "#Import Glove class\n",
    "from glove import Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create instance of glove\n",
    "glove = Glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kojguGTahPv"
   },
   "outputs": [],
   "source": [
    "#Iterator for given file\n",
    "def iterate_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f, delimiter=\",\")\n",
    "        for row in reader:\n",
    "            yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5pvqkbFj8zC"
   },
   "outputs": [],
   "source": [
    "# get positive and negative stars for every word\n",
    "# get percentage of unkown words for vocab\n",
    "\n",
    "count_wordsinvocab = 0\n",
    "count_words = 0\n",
    "count_questions_with_unkown = 0\n",
    "count_questions = 0\n",
    "weights_directory = {}\n",
    "unkown_counter = {} \n",
    "\n",
    "i = 0\n",
    "for row in iterate_file(\"../processed_files/english_train.csv\"):\n",
    "    i += 1\n",
    "    words = sentence.split(\" \")\n",
    "    one_unkown = False\n",
    "    count_questions += 1\n",
    "    for word in words:\n",
    "        if word != \"\":\n",
    "            count_words += 1\n",
    "            if count_words % 1000000==0:\n",
    "                print(\"words:\",count_words)\n",
    "            if word in glove.embeddings:\n",
    "                count_wordsinvocab +=1\n",
    "            else:\n",
    "                unkown_counter[word] = unkown_counter.get(word, 0) + 1\n",
    "                one_unkown=True\n",
    "        \n",
    "    count_questions_with_unkown += one_unkown\n",
    "\n",
    "unkown_counts = [count for count in unkown_counter.values()]\n",
    "unkown_counts.sort()\n",
    "\n",
    "for count in unkown_counts[-10:]:\n",
    "    print(count, [word for word,counti in unkown_counter.items() if count==counti])\n",
    "\n",
    "print(\"percentage of words in vocab:\", count_wordsinvocab / count_words)\n",
    "print(\"percentage of question with unknown words:\", count_questions_with_unkown/count_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CUH7vsqraVsy"
   },
   "outputs": [],
   "source": [
    "#Find relation between dates and score of questions\n",
    "#Create dictionary of years\n",
    "years = {}\n",
    "\n",
    "#Reads input file and updates dates inside the dictionary\n",
    "def update_dates(filename):\n",
    "    #Iterate through file\n",
    "    for row in iterate_file(filename):\n",
    "        #Get year data\n",
    "        year = int(row[\"creation_date\"].split(\"T\")[0].split(\"-\")[0])\n",
    "        #Append year into dictionary with its score\n",
    "        if year in years:\n",
    "            years[year].append(int(row[\"score\"]))\n",
    "        else:\n",
    "            years[year] = [(int(row[\"score\"]))]\n",
    "\n",
    "#Read all 3 files to fill up years dictionary\n",
    "update_dates(\"../processed_files/english_train.csv\")\n",
    "update_dates(\"../processed_files/english_test.csv\")\n",
    "update_dates(\"../processed_files/english_test.csv\")\n",
    "\n",
    "means = [np.mean(scores) for year, scores in years.items()]\n",
    "\n",
    "plt.plot(list(years.keys()), means)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Mean Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
