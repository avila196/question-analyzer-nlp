{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model - NN with Word Embeddings from GloVe\n",
    "### Dataset: Stack Overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qNyjMARx4jtH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.1.0\n",
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#Imports for model\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import csv\n",
    "from tensorflow import keras\n",
    "\n",
    "#Import GloVe model\n",
    "from glove import Glove\n",
    "\n",
    "print(\"TensorFlow Version: \"+tf.__version__)\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"GPU not found. Please install GPU version of TF if needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BIclN-yggl6",
    "outputId": "743c4bd5-c8de-4329-9fd6-9dae8915efbf"
   },
   "outputs": [],
   "source": [
    "#Install import_ipynb to import other notebooks\n",
    "!pip install import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KbG33nrjhAZ"
   },
   "outputs": [],
   "source": [
    "#Download and prepare the Pre-trained GloVe Word Embedding model\n",
    "path_to_glove_zipfile = \"../processed_files/glove.42B.300d.zip\"\n",
    "path_to_glove_file = \"../processed_files/glove.42B.300d.txt\"\n",
    "\n",
    "if not path.exists(path_to_glove_file):\n",
    "    if not path.exists(path_to_glove_zipfile):\n",
    "        print(\"downloading glove .zip file...\")\n",
    "        !wget http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
    "    print(\"unzipping glove .zip file...\")\n",
    "    !unzip -q glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uoFD4bQK3oqj",
    "outputId": "cc933c19-e7d4-458e-eeb8-f51a407eb039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Create instance of glove. Can take up to 5mins to load all pre-trained word embeddings for GloVe model\n",
    "glove = Glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.0242e-01 -3.5931e-01 -6.5666e-01  1.6470e-01  2.2212e-01  3.2755e-01\n",
      " -9.8938e-01  1.3407e+00  3.3532e-02 -4.3492e-01 -1.1260e-01 -9.6771e-02\n",
      " -8.2175e-01  1.0123e+00 -6.2944e-01 -1.2833e-01  7.6772e-01 -2.9737e-01\n",
      "  6.3013e-01 -5.2358e-01  2.1238e-01  7.7167e-02  5.0815e-01  4.8051e-03\n",
      "  6.6603e-02  6.4908e-01  4.9159e-01 -4.5719e-01 -4.3848e-01 -5.1041e-01\n",
      " -3.9617e-01 -4.4244e-01  1.2044e+00  9.1132e-02 -3.6845e-01 -2.0362e-01\n",
      "  1.5433e-01  6.5747e-01 -3.1456e-01  9.7153e-01 -6.3147e-01  1.0481e-02\n",
      " -4.7715e-01  4.7417e-01 -2.6940e-01 -4.5268e-01  2.1765e-01  1.5206e-01\n",
      "  1.8309e-01 -1.6915e-01  2.3382e-02  8.2740e-01  3.9396e-01 -8.1216e-02\n",
      " -1.5340e-01  2.9491e-01  1.9455e-02 -1.7298e-01 -2.4993e-01  3.2447e-01\n",
      "  8.3227e-01  7.6610e-02  1.7777e-01  2.8370e-01  1.8154e-01 -3.5773e-01\n",
      " -4.7704e-01  1.6308e-01 -5.6907e-02  3.7091e-01 -1.7129e-01  5.7642e-01\n",
      "  6.3547e-01  2.0492e-01 -4.5779e-01 -9.6861e-02 -6.2884e-01  1.9092e-01\n",
      " -1.4184e-01 -1.6334e-01 -4.7154e-02 -8.3175e-02 -3.3505e-01 -4.6953e-01\n",
      " -3.5504e-01  4.4959e-01 -4.1630e-01 -2.9431e-01 -1.8715e-02  9.0586e-01\n",
      "  4.9707e-01 -6.5061e-01  4.8165e-01 -5.4500e-01  1.1014e+00  3.2310e-01\n",
      " -7.6509e-01 -7.9294e-02  3.0798e-03 -9.9578e-02 -3.2798e-01 -3.0389e-02\n",
      " -1.4883e-01  2.6812e-01  4.2039e-01 -6.0034e-01  1.5038e-01  2.6113e-01\n",
      " -4.8786e-02 -3.5999e-01  9.7600e-02  2.1247e-01 -7.9939e-02  8.7059e-02\n",
      " -2.7054e-01 -8.9917e-03 -2.9726e-01 -4.1830e-01  3.6757e-01 -7.9944e-02\n",
      "  4.6576e-01  5.0479e-01  7.7166e-01 -5.1801e-01  9.8158e-01 -6.1025e-01\n",
      " -3.4432e-01 -2.1683e-01  2.8706e-01  7.2464e-01 -4.5971e-01  2.0568e-01\n",
      "  3.8584e-01 -2.3031e-01 -8.5781e-02  2.3061e-01  7.5603e-02  8.2191e-02\n",
      "  1.8811e-01  2.9041e-01  3.9689e-01 -1.5018e-02  4.2691e-01 -1.6345e-01\n",
      "  5.3200e-01 -2.1953e-01  6.1068e-01 -4.8974e-01 -7.6076e-01 -2.3474e-01\n",
      "  1.3115e-01  2.8908e-01  8.1798e-01  2.3084e-01  6.5341e-01  2.4453e-01\n",
      "  7.0238e-01  3.6114e-01  3.6579e-02  4.0010e-01 -5.2601e-01  2.7759e-01\n",
      "  5.2295e-02  6.3001e-03 -9.0928e-02 -6.4573e-01 -2.1717e-01 -3.4593e-01\n",
      " -3.9445e-01  3.0251e-02 -2.3378e-01  1.5594e-01  1.5806e-02 -6.6592e-01\n",
      "  6.8529e-01  3.4757e-01  3.8451e-01 -1.7536e-01 -5.9741e-01  3.1796e-01\n",
      " -4.5363e-01  4.3917e-02 -7.0287e-02 -7.4374e-02  1.4001e-01 -5.8610e-01\n",
      "  5.2160e-01 -7.1753e-01 -4.7402e-01  5.9690e-01  3.3364e-01  6.5212e-01\n",
      "  2.7808e-01  7.2654e-01  7.0927e-02 -4.2427e-01  1.5539e-01 -5.6488e-04\n",
      " -4.8350e-01 -1.3119e+00  2.4483e-02  6.9947e-01 -7.8485e-02 -2.7598e-01\n",
      " -1.9552e-01  7.7246e-01 -9.9199e-01 -1.8835e-01 -1.1349e-02  4.8694e-01\n",
      "  3.8786e-01 -3.5498e-01  4.4483e-01 -3.4895e-01  7.8352e-02 -1.6382e-01\n",
      " -4.0711e-01  9.4917e-02 -2.4694e-01  4.2897e-01 -5.7419e-01 -1.3331e-01\n",
      " -6.3825e-01  6.0159e-01 -7.3611e-01  3.3953e-01  1.0755e-01 -4.4161e-01\n",
      " -5.1621e-01  9.2538e-01 -1.7751e-02  1.4353e-01  1.0766e-01 -6.3709e-01\n",
      "  1.9061e-01 -3.3838e-01  4.6853e-02  1.6434e-01 -6.8421e-01 -6.7776e-01\n",
      "  2.9103e-01 -1.2233e-01 -1.5557e-01 -1.2949e+00 -4.5583e-01 -6.3864e-01\n",
      "  4.5638e-01 -1.1198e-01  3.9628e-01  1.3045e-01  3.4738e-02 -5.7978e-01\n",
      " -5.6770e-02  3.9230e-01 -7.0704e-01  6.5727e-01  4.2612e-01  6.3776e-01\n",
      " -5.0650e-01  2.8172e-02 -1.4630e-01 -1.2322e-01  2.9535e-01 -2.9081e-01\n",
      "  4.6450e-01 -6.1105e-01 -2.4137e-01 -2.0506e-01  2.7220e-01 -1.7796e-01\n",
      "  4.9109e-01 -1.1448e-01 -3.3460e-01  2.5017e-01 -5.5843e-02  1.7946e-01\n",
      "  3.2894e-01  4.3637e-01  2.9061e-01 -2.5199e-03  7.9951e-02  1.8228e-02\n",
      "  2.1791e-01  3.9580e-01 -1.0274e-01 -1.3623e-01 -9.8154e-02 -3.5365e-01\n",
      " -2.4702e-01 -6.8414e-01  2.6346e-01 -2.8685e-01  5.5017e-01  4.3039e-01\n",
      " -3.3990e-01  4.3356e-01  7.8930e-01  6.3652e-03  1.5180e-01  6.3343e-01]\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "#Check some word vector representations\n",
    "print(glove.vector(\"decimal\"))\n",
    "print(glove.vector(\"nhibernate\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwGFvAe59ooM"
   },
   "outputs": [],
   "source": [
    "#Define the input data for training as batches\n",
    "#For input file, the label is given by the \"stars\" column, which is the 3rd col\n",
    "#Generator of batches\n",
    "def batch_generator(train_df,batch_size,steps):\n",
    "    idx=1\n",
    "    while True: \n",
    "        yield load_data(train_df,idx-1,batch_size) # Yields data\n",
    "        if idx < steps:\n",
    "            idx+=1\n",
    "        else:\n",
    "            idx=1\n",
    "\n",
    "#Loads the requested batch given its index\n",
    "def load_data(train_df,idx,batch_size):\n",
    "    df = pd.read_csv(train_df, skiprows=idx*batch_size,nrows=batch_size)\n",
    "    x = df.iloc[:,1:]\n",
    "    y = df.iloc[:,0]\n",
    "    return (np.array(x), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGP8qom0LWev"
   },
   "outputs": [],
   "source": [
    "#Sentence to sequence vectors\n",
    "def convert_to_vec(sentence, max_sequence_length, vec_dim):\n",
    "    words = sentence.split(\" \")\n",
    "    vec = np.zeros((max_sequence_length, vec_dim))\n",
    "    for i in range(max_sequence_length):\n",
    "        if i == len(words):\n",
    "            break\n",
    "        vec[i] = glove.vector(words[i])\n",
    "    return vec\n",
    "\n",
    "#Read CSV and return X and Y matrix\n",
    "def read_and_parse(file_path, input_size=float(\"inf\"), max_sequence_length=30, vec_dim=300):\n",
    "    matrix = []\n",
    "    labels = []\n",
    "    with open(file_path, \"r\", encoding = 'utf8') as f:\n",
    "        reader = csv.DictReader(f, delimiter=\",\")\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            question = row[\"title\"]\n",
    "            vec = convert_to_vec(question, max_sequence_length, vec_dim)\n",
    "            matrix.append(vec)\n",
    "            # labels[i] = float(row[\"stars\"])\n",
    "            labels.append(float(row[\"stars\"])>3)\n",
    "            i += 1\n",
    "            if i==input_size:\n",
    "                break\n",
    "        return np.array(matrix), np.array(labels)\n",
    "\n",
    "#Parameters\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "VEC_DIM = 300\n",
    "INPUT_SIZE_TRAIN = 27000\n",
    "INPUT_SIZE_TEST = 1500\n",
    "INPUT_FILE_TRAIN = \"data_stackoverflow_train.csv\"\n",
    "INPUT_FILE_TEST = \"data_stackoverflow_test.csv\"\n",
    "INPUT_FILE_VAL = \"data_stackoverflow_val.csv\"\n",
    "\n",
    "#Read CSV for training\n",
    "input_sequences_train,labels_train = read_and_parse(INPUT_FILE_TRAIN, max_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#Read CSV and create input matrix for testing\n",
    "input_sequences_test,labels_test = read_and_parse(INPUT_FILE_TEST, max_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#Read CSV and create input matrix for validation\n",
    "input_sequences_val,labels_val = read_and_parse(INPUT_FILE_VAL, max_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PasDsA6vB_Jb",
    "outputId": "e7b60bbf-b962-41b4-facf-730c3c6cd10f"
   },
   "outputs": [],
   "source": [
    "print(\"input shape:\",input_sequences_train.shape)\n",
    "print(\"labels train:\",labels_train.shape)\n",
    "print(\"input shape:\",input_sequences_test.shape)\n",
    "print(\"labels train:\",labels_test.shape)\n",
    "labels_train[0]\n",
    "print(\"trues over all:\", sum(labels_train)/len(labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mGgdlYv0OSp",
    "outputId": "81134e30-fef5-45d6-f56a-bbc991f8f7f4"
   },
   "outputs": [],
   "source": [
    "# Create model #1\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D, Flatten, Dense, Bidirectional, Input, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# inputs: A 3D tensor with shape [batch, timesteps, feature].\n",
    "# inputs = tf.random.normal([32, 10, 8])\n",
    "# lstm = tf.keras.layers.LSTM(4)\n",
    "# output = lstm(inputs)\n",
    "# print(output.shape) = (32,4)\n",
    "\n",
    "input = Input(shape=(MAX_SEQUENCE_LENGTH,glove.dim))\n",
    "# X = SpatialDropout1D(0.4)(Input)\n",
    "x = Bidirectional(LSTM(256, activation=\"relu\", return_sequences=True))(input)\n",
    "# x = Dropout(0.2)(x)\n",
    "x = SpatialDropout1D(0.2)(x)\n",
    "# x = Bidirectional(LSTM(128, activation=\"relu\", return_sequences=True))(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "x = LSTM(256, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "x = Flatten()(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "x = Dense(1024 ,activation=\"relu\")(x)\n",
    "x = Dense(128 ,activation=\"relu\")(x)\n",
    "# preds = Dense(1)(x)\n",
    "preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model1 = Model(input, preds)\n",
    "model1.compile(loss = keras.losses.binary_crossentropy,\n",
    "              # loss='mean_squared_error',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.0003),# 0.0003\n",
    "              # metrics=[\"mean_absolute_error\"])\n",
    "              metrics=[\"accuracy\"])\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5-hAMtNFje3E",
    "outputId": "529214a8-d200-42cf-fb87-e403f8372cda"
   },
   "outputs": [],
   "source": [
    "#Fit model\n",
    "model1.fit(input_sequences_train, labels_train, validation_data=(input_sequences_test, labels_test), epochs=50, batch_size=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZeBDT9dkmN37",
    "outputId": "1407371c-99ae-4e4a-e45b-ccc68bbcf4fa"
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "              \"compressing / decompressing folders & files\",\n",
    "              \"HOW TO decompress and compress files and folders\",\n",
    "              \"how to load a specific version of an assembly\",\n",
    "              \"how would one code test and set behavior without a special hardware instruction?\",\n",
    "             \"can you debug a .net app with only the source code of one file?\",\n",
    "             \"what columns generally make good indexes?\",\n",
    "             \"why is there no generic synchronized queue in .net?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    input_sample = convert_to_vec(question, MAX_SEQUENCE_LENGTH, VEC_DIM)\n",
    "    input_sample = input_sample[np.newaxis,...]\n",
    "    print(model1.predict(input_sample))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMInEdSQ_8xB",
    "outputId": "c66a0916-14a9-4f45-b2b4-2d215c31a02b"
   },
   "outputs": [],
   "source": [
    "predictions = model1.predict(input_sequences_test)\n",
    "for y_hat,y  in zip(predictions[:50], labels_test[:50]):\n",
    "    print(y,\":\", y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtuCYGZBdu8t"
   },
   "outputs": [],
   "source": [
    "#create word to index dictionary: word->index\n",
    "# index 0 is for unnexistent word\n",
    "word_index = {}\n",
    "cont = 0\n",
    "for word in glove.embeddings.keys():\n",
    "    cont +=1\n",
    "    word_index[word] = cont\n",
    "\n",
    "#Processing of GloVe data into the embedding matrix to use in Keras\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, glove.dim))\n",
    "EMBEDDING_DIM = glove.dim\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_matrix[i] = glove.vector(word)\n",
    "\n",
    "# delete glove dictionary to save memory RAM\n",
    "#del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zS7JR6Rix9UU"
   },
   "outputs": [],
   "source": [
    "# Create model #1\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "#del embedding_matrix\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(16, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(16, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(16, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(1, activation='relu')(x)\n",
    "\n",
    "model1 = Model(sequence_input, preds)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "# model1.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "#           epochs=2, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9UrFm5xyGBz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
