{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qNyjMARx4jtH"
   },
   "outputs": [],
   "source": [
    "#Imports for model\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "#Install import_ipynb to import other notebooks\n",
    "# !pip install import_ipynb\n",
    "import import_ipynb\n",
    "import tensorflow as tf\n",
    "# physical_devices = tf.config.list_physical_devices('GPU') \n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BIclN-yggl6",
    "outputId": "e956048f-12bb-4495-fafe-8f5e3d96c9bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4KbG33nrjhAZ"
   },
   "outputs": [],
   "source": [
    "#Import Glove class\n",
    "from glove import Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9C_hrDmYF6Op",
    "outputId": "90be82f5-51a5-46c4-d4d5-490a007d3155"
   },
   "outputs": [],
   "source": [
    "#Create instance of glove\n",
    "glove = Glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpimHx-DF6Oq",
    "outputId": "16d52b39-9c64-4fb2-c035-69705e230c2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.vector(\"__OOV__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cM9P8LUUF6Oq"
   },
   "outputs": [],
   "source": [
    "#Clean sentence (remove non alpha chars)\n",
    "p = re.compile(r'<.*?>')\n",
    "def cleanSentence(sentence, html = False):\n",
    "    if html: sentence = p.sub('', sentence) \n",
    "    sentence = ''.join([i.lower() if i.isalpha() else \" \" if (i==\" \" or i==\"-\" or i==\"_\") else \"\" for i in sentence])\n",
    "    return sentence\n",
    "\n",
    "#Iterator for given file\n",
    "def iterate_file(file_path):\n",
    "    with open(file_path, \"r\", encoding = 'utf8') as f:\n",
    "        reader = csv.DictReader(f, delimiter=\",\")\n",
    "        for row in reader:\n",
    "            yield row\n",
    "\n",
    "def convert_strlist_to_vec(strlist, max_sequence_length, vec_dim):\n",
    "    vec = np.zeros((max_sequence_length, vec_dim))\n",
    "    for i in range(max_sequence_length):\n",
    "        if i == len(strlist):\n",
    "            break\n",
    "        vec[i] = glove.vector(strlist[i])\n",
    "    return vec\n",
    "\n",
    "# def data_generator(file_path, minibatch_size=5000, max_sequence_length=15, vec_dim=300, Infinite_loop=True):\n",
    "#     i = 0\n",
    "#     X = []\n",
    "#     Y = []\n",
    "#     while True:\n",
    "#         for row in iterate_file(file_path):\n",
    "#             info = row[\"tags\"].split(\"|\") + row[\"title\"].split()\n",
    "#             xi = convert_strlist_to_vec(info, max_sequence_length, vec_dim)\n",
    "#             yi =  (float(row[\"stars\"])>3)\n",
    "#             X.append(xi)\n",
    "#             Y.append(yi)\n",
    "#             i +=1\n",
    "#             if i>=minibatch_size:\n",
    "#                 yield np.array(X), np.array(Y)#, [None]\n",
    "#                 i = 0\n",
    "#                 X = []\n",
    "#                 Y = []\n",
    "#         if not Infinite_loop: break\n",
    "\n",
    "def data_generator(file_path, minibatch_size=5000, max_sequence_length=15, vec_dim=300, Infinite_loop=True):\n",
    "    i = 0\n",
    "    X1 = []\n",
    "    X2= []\n",
    "    Y = []\n",
    "    while True:\n",
    "        for row in iterate_file(file_path):\n",
    "            #----------------\n",
    "            info = row[\"tags\"].split(\"|\") + [\".\"] +row[\"title\"].split() #+ [\".\"] + row[\"body\"].split()\n",
    "            x1i = convert_strlist_to_vec(info, max_sequence_length, vec_dim)\n",
    "            #----------------\n",
    "            date = row[\"creation_date\"]\n",
    "            x2i = [0]*10\n",
    "            x2i[int(date[3])] = 1\n",
    "#             print(date, x2i)\n",
    "            #----------------\n",
    "            yi =  (float(row[\"stars\"])>3)\n",
    "            #----------------\n",
    "            X1.append(x1i)\n",
    "            X2.append(x2i)\n",
    "            Y.append(yi)\n",
    "            i +=1\n",
    "            if i>=minibatch_size:\n",
    "                yield [np.array(X1), np.array(X2)], np.array(Y)#, [None]\n",
    "                i = 0\n",
    "                X1 = []\n",
    "                X2 = []\n",
    "                Y = []\n",
    "        if not Infinite_loop: break\n",
    "\n",
    "# def make_gen_callable(_gen):\n",
    "#         def gen():\n",
    "#             for x,y in _gen:\n",
    "#                  yield x,y\n",
    "#         return gen\n",
    "        \n",
    "# def create_dataset(file_path, minibatch_size=3000, max_sequence_length=15):\n",
    "#     generator = make_gen_callable(data_generator(file_path, minibatch_size, max_sequence_length))\n",
    "#     dataset = tf.data.Dataset.from_generator(generator=generator,\n",
    "#                                              output_types=(tf.float32,tf.bool),\n",
    "#                                              output_shapes = ((None,max_sequence_length,300), (None,))\n",
    "#                                             )\n",
    "#     return dataset\n",
    "        \n",
    "# #Parameters\n",
    "MAX_SEQUENCE_LENGTH = 15\n",
    "VEC_DIM = 300\n",
    "INPUT_FILE_TRAIN = \"../processed_files/data_stackoverflow_train.csv\"\n",
    "INPUT_FILE_VAL = \"../processed_files/data_stackoverflow__val.csv\"\n",
    "INPUT_FILE_TEST = \"../processed_files/data_stackoverflow__test.csv\"\n",
    "\n",
    "train_dataset_size = 900000  #<<<---------------OJO!! xoxo\n",
    "val_dataset_size = 50000\n",
    "\n",
    "minibatch_size = 512\n",
    "train_generator = data_generator(INPUT_FILE_TRAIN, minibatch_size, MAX_SEQUENCE_LENGTH)\n",
    "val_generator = data_generator(INPUT_FILE_VAL, minibatch_size, MAX_SEQUENCE_LENGTH)\n",
    "# train_generator = create_dataset(INPUT_FILE_TRAIN, minibatch_size, MAX_SEQUENCE_LENGTH)\n",
    "# val_generator = create_dataset(INPUT_FILE_VAL, minibatch_size, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "# cont = 0\n",
    "# for data in data_generator(INPUT_FILE_VAL):\n",
    "#     data\n",
    "#     cont +=1\n",
    "#     if cont >= 10:\n",
    "#         break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mGgdlYv0OSp",
    "outputId": "42411d36-277a-44fb-ab68-3409ff41df55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_28 (InputLayer)           [(None, 30, 300)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 30, 128)      186880      input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_27 (LSTM)                  (None, 64)           49408       bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_13 (Flatten)            (None, 64)           0           lstm_27[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_29 (InputLayer)           [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 74)           0           flatten_13[0][0]                 \n",
      "                                                                 input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 256)          19200       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 128)          32896       dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 1)            129         dense_31[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 288,513\n",
      "Trainable params: 288,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model #1\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import Input, LSTM, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Bidirectional, Dropout, SpatialDropout1D, Concatenate\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# inputs: A 3D tensor with shape [batch, timesteps, feature].\n",
    "# inputs = tf.random.normal([32, 10, 8])\n",
    "\n",
    "# X = SpatialDropout1D(0.4)(Input)\n",
    "# x = LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "# x = Dropout(0.2)(x)\n",
    "#----------------------------------------------------------------------\n",
    "input1 = Input(shape=(MAX_SEQUENCE_LENGTH,300))\n",
    "input2 = Input(shape= (10,))\n",
    "x = Bidirectional(LSTM(64, activation=\"relu\", return_sequences=True))(input1)\n",
    "x = LSTM(64, activation=\"relu\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Concatenate()([x, input2])\n",
    "x = Dense(256 ,activation=\"relu\")(x)\n",
    "x = Dense(128 ,activation=\"relu\")(x)\n",
    "# preds = Dense(1)(x)\n",
    "preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model1 = Model([input1, input2], preds)\n",
    "model1.compile(loss = keras.losses.binary_crossentropy,\n",
    "              # loss='mean_squared_error',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.0003),# 0.0003\n",
    "              # metrics=[\"mean_absolute_error\"])\n",
    "              metrics=[\"accuracy\"])\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-hAMtNFje3E",
    "outputId": "be616f5e-a475-46ef-9a19-bd920b4e9acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1757 steps, validate for 97 steps\n",
      "Epoch 1/5\n",
      "1757/1757 [==============================] - 91s 52ms/step - loss: 0.6600 - accuracy: 0.6053 - val_loss: 0.6562 - val_accuracy: 0.6114\n",
      "Epoch 2/5\n",
      "1757/1757 [==============================] - 86s 49ms/step - loss: 0.6516 - accuracy: 0.6173 - val_loss: 0.6529 - val_accuracy: 0.6181\n",
      "Epoch 3/5\n",
      "1757/1757 [==============================] - 88s 50ms/step - loss: 0.6478 - accuracy: 0.6225 - val_loss: 0.6521 - val_accuracy: 0.6207\n",
      "Epoch 4/5\n",
      "1757/1757 [==============================] - 88s 50ms/step - loss: 0.6448 - accuracy: 0.6267 - val_loss: 0.6518 - val_accuracy: 0.6218\n",
      "Epoch 5/5\n",
      "1757/1757 [==============================] - 85s 48ms/step - loss: 0.6420 - accuracy: 0.6306 - val_loss: 0.6519 - val_accuracy: 0.6213\n"
     ]
    }
   ],
   "source": [
    "#Fit model\n",
    "# model1.fit(input_sequences_train, labels_train, validation_data=(input_sequences_test, labels_test), epochs=50, batch_size=3000)\n",
    "history = model1.fit( \n",
    "            train_generator,\n",
    "            validation_data = val_generator,\n",
    "            steps_per_epoch = train_dataset_size//minibatch_size,\n",
    "            validation_steps = val_dataset_size//minibatch_size,\n",
    "            epochs=5,\n",
    "            use_multiprocessing=False)\n",
    "# model1.fit_generator(train_dataset, validation_data=test_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeBDT9dkmN37"
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "              \"compressing / decompressing folders & files\",\n",
    "              \"HOW TO decompress and compress files and folders\",\n",
    "              \"how to load a specific version of an assembly\",\n",
    "              \"how would one code test and set behavior without a special hardware instruction?\",\n",
    "             \"can you debug a .net app with only the source code of one file?\",\n",
    "             \"what columns generally make good indexes?\",\n",
    "             \"why is there no generic synchronized queue in .net?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    input_sample = convert_to_vec(question, MAX_SEQUENCE_LENGTH, VEC_DIM)\n",
    "    input_sample = input_sample[np.newaxis,...]\n",
    "    print(model1.predict(input_sample))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMInEdSQ_8xB",
    "outputId": "c66a0916-14a9-4f45-b2b4-2d215c31a02b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.155926911907669 : [2.437947]\n",
      "5.433777912584217 : [3.633104]\n",
      "2.0017196115086326 : [2.5025215]\n",
      "5.222153851359976 : [4.8705235]\n",
      "4.507115698179441 : [2.7976453]\n",
      "4.507115698179441 : [1.7287428]\n",
      "4.868024606543382 : [4.4408703]\n",
      "5.267909551072163 : [3.3222017]\n",
      "4.868024606543382 : [3.9014714]\n",
      "3.71046691271883 : [2.9469485]\n",
      "5.452131673820725 : [4.4914856]\n",
      "5.385722223148394 : [3.0718992]\n",
      "5.374769434956709 : [3.9768033]\n",
      "5.301211068316426 : [1.9733393]\n",
      "5.3616434320469795 : [1.841399]\n",
      "2.0017196115086326 : [1.6214281]\n",
      "4.868024606543382 : [3.82397]\n",
      "3.71046691271883 : [1.7828467]\n",
      "3.71046691271883 : [3.4396913]\n",
      "4.507115698179441 : [3.6329727]\n",
      "2.0017196115086326 : [2.6412437]\n",
      "3.71046691271883 : [4.7605033]\n",
      "5.222153851359976 : [4.1931186]\n",
      "3.71046691271883 : [2.711287]\n",
      "2.0017196115086326 : [2.1272194]\n",
      "5.301211068316426 : [3.9586036]\n",
      "3.71046691271883 : [2.2625918]\n",
      "5.051496032153824 : [4.1617684]\n",
      "5.486144574266142 : [2.4890935]\n",
      "3.71046691271883 : [2.259145]\n",
      "3.71046691271883 : [1.8665693]\n",
      "3.71046691271883 : [1.6438717]\n",
      "2.0017196115086326 : [3.0529742]\n",
      "5.155926911907669 : [1.905072]\n",
      "5.47873638537213 : [3.323313]\n",
      "3.71046691271883 : [2.1089969]\n",
      "0.5501842405356051 : [3.8193178]\n",
      "5.051496032153824 : [1.766904]\n",
      "5.326463056967139 : [3.817408]\n",
      "4.507115698179441 : [3.9802637]\n",
      "5.461667791758554 : [3.6945252]\n",
      "4.507115698179441 : [2.77272]\n",
      "2.0017196115086326 : [3.3238227]\n",
      "4.868024606543382 : [1.7591114]\n",
      "3.71046691271883 : [4.700082]\n",
      "2.0017196115086326 : [2.4840746]\n",
      "2.0017196115086326 : [2.4377687]\n",
      "4.868024606543382 : [1.7795018]\n",
      "4.868024606543382 : [2.4089942]\n",
      "4.868024606543382 : [1.6272384]\n"
     ]
    }
   ],
   "source": [
    "predictions = model1.predict(input_sequences_test)\n",
    "for y_hat,y  in zip(predictions[:50], labels_test[:50]):\n",
    "    print(y,\":\", y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtuCYGZBdu8t"
   },
   "outputs": [],
   "source": [
    "#create word to index dictionary: word->index\n",
    "# index 0 is for unnexistent word\n",
    "word_index = {}\n",
    "cont = 0\n",
    "for word in glove.embeddings.keys():\n",
    "    cont +=1\n",
    "    word_index[word] = cont\n",
    "\n",
    "#Processing of GloVe data into the embedding matrix to use in Keras\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, glove.dim))\n",
    "EMBEDDING_DIM = glove.dim\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_matrix[i] = glove.vector(word)\n",
    "\n",
    "# delete glove dictionary to save memory RAM\n",
    "#del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zS7JR6Rix9UU"
   },
   "outputs": [],
   "source": [
    "# Create model #1\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "#del embedding_matrix\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(16, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(16, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(16, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(1, activation='relu')(x)\n",
    "\n",
    "model1 = Model(sequence_input, preds)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "# model1.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "#           epochs=2, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9UrFm5xyGBz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model_4v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
