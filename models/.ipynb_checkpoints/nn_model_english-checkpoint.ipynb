{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Model - NN with Word Embeddings from GloVe\n",
    "### Dataset: English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qNyjMARx4jtH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.3.1\n",
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#Imports for model\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import csv\n",
    "from tensorflow import keras\n",
    "\n",
    "#Import GloVe model\n",
    "from glove import Glove\n",
    "\n",
    "print(\"TensorFlow Version: \"+tf.__version__)\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"GPU not found. Please install GPU version of TF if needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BIclN-yggl6",
    "outputId": "1b200075-8652-429e-d8e1-ca811cce2933"
   },
   "outputs": [],
   "source": [
    "#Install import_ipynb to import other notebooks\n",
    "!pip install import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download and prepare the Pre-trained GloVe Word Embedding model\n",
    "path_to_glove_zipfile = \"../processed_files/glove.42B.300d.zip\"\n",
    "path_to_glove_file = \"../processed_files/glove.42B.300d.txt\"\n",
    "\n",
    "if not path.exists(path_to_glove_file):\n",
    "    if not path.exists(path_to_glove_zipfile):\n",
    "        print(\"downloading glove .zip file...\")\n",
    "        !wget http://nlp.stanford.edu/data/glove.42B.300d.zip\n",
    "    print(\"unzipping glove .zip file...\")\n",
    "    !unzip -q glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4KbG33nrjhAZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Create instance of glove\n",
    "glove = Glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.0242e-01 -3.5931e-01 -6.5666e-01  1.6470e-01  2.2212e-01  3.2755e-01\n",
      " -9.8938e-01  1.3407e+00  3.3532e-02 -4.3492e-01 -1.1260e-01 -9.6771e-02\n",
      " -8.2175e-01  1.0123e+00 -6.2944e-01 -1.2833e-01  7.6772e-01 -2.9737e-01\n",
      "  6.3013e-01 -5.2358e-01  2.1238e-01  7.7167e-02  5.0815e-01  4.8051e-03\n",
      "  6.6603e-02  6.4908e-01  4.9159e-01 -4.5719e-01 -4.3848e-01 -5.1041e-01\n",
      " -3.9617e-01 -4.4244e-01  1.2044e+00  9.1132e-02 -3.6845e-01 -2.0362e-01\n",
      "  1.5433e-01  6.5747e-01 -3.1456e-01  9.7153e-01 -6.3147e-01  1.0481e-02\n",
      " -4.7715e-01  4.7417e-01 -2.6940e-01 -4.5268e-01  2.1765e-01  1.5206e-01\n",
      "  1.8309e-01 -1.6915e-01  2.3382e-02  8.2740e-01  3.9396e-01 -8.1216e-02\n",
      " -1.5340e-01  2.9491e-01  1.9455e-02 -1.7298e-01 -2.4993e-01  3.2447e-01\n",
      "  8.3227e-01  7.6610e-02  1.7777e-01  2.8370e-01  1.8154e-01 -3.5773e-01\n",
      " -4.7704e-01  1.6308e-01 -5.6907e-02  3.7091e-01 -1.7129e-01  5.7642e-01\n",
      "  6.3547e-01  2.0492e-01 -4.5779e-01 -9.6861e-02 -6.2884e-01  1.9092e-01\n",
      " -1.4184e-01 -1.6334e-01 -4.7154e-02 -8.3175e-02 -3.3505e-01 -4.6953e-01\n",
      " -3.5504e-01  4.4959e-01 -4.1630e-01 -2.9431e-01 -1.8715e-02  9.0586e-01\n",
      "  4.9707e-01 -6.5061e-01  4.8165e-01 -5.4500e-01  1.1014e+00  3.2310e-01\n",
      " -7.6509e-01 -7.9294e-02  3.0798e-03 -9.9578e-02 -3.2798e-01 -3.0389e-02\n",
      " -1.4883e-01  2.6812e-01  4.2039e-01 -6.0034e-01  1.5038e-01  2.6113e-01\n",
      " -4.8786e-02 -3.5999e-01  9.7600e-02  2.1247e-01 -7.9939e-02  8.7059e-02\n",
      " -2.7054e-01 -8.9917e-03 -2.9726e-01 -4.1830e-01  3.6757e-01 -7.9944e-02\n",
      "  4.6576e-01  5.0479e-01  7.7166e-01 -5.1801e-01  9.8158e-01 -6.1025e-01\n",
      " -3.4432e-01 -2.1683e-01  2.8706e-01  7.2464e-01 -4.5971e-01  2.0568e-01\n",
      "  3.8584e-01 -2.3031e-01 -8.5781e-02  2.3061e-01  7.5603e-02  8.2191e-02\n",
      "  1.8811e-01  2.9041e-01  3.9689e-01 -1.5018e-02  4.2691e-01 -1.6345e-01\n",
      "  5.3200e-01 -2.1953e-01  6.1068e-01 -4.8974e-01 -7.6076e-01 -2.3474e-01\n",
      "  1.3115e-01  2.8908e-01  8.1798e-01  2.3084e-01  6.5341e-01  2.4453e-01\n",
      "  7.0238e-01  3.6114e-01  3.6579e-02  4.0010e-01 -5.2601e-01  2.7759e-01\n",
      "  5.2295e-02  6.3001e-03 -9.0928e-02 -6.4573e-01 -2.1717e-01 -3.4593e-01\n",
      " -3.9445e-01  3.0251e-02 -2.3378e-01  1.5594e-01  1.5806e-02 -6.6592e-01\n",
      "  6.8529e-01  3.4757e-01  3.8451e-01 -1.7536e-01 -5.9741e-01  3.1796e-01\n",
      " -4.5363e-01  4.3917e-02 -7.0287e-02 -7.4374e-02  1.4001e-01 -5.8610e-01\n",
      "  5.2160e-01 -7.1753e-01 -4.7402e-01  5.9690e-01  3.3364e-01  6.5212e-01\n",
      "  2.7808e-01  7.2654e-01  7.0927e-02 -4.2427e-01  1.5539e-01 -5.6488e-04\n",
      " -4.8350e-01 -1.3119e+00  2.4483e-02  6.9947e-01 -7.8485e-02 -2.7598e-01\n",
      " -1.9552e-01  7.7246e-01 -9.9199e-01 -1.8835e-01 -1.1349e-02  4.8694e-01\n",
      "  3.8786e-01 -3.5498e-01  4.4483e-01 -3.4895e-01  7.8352e-02 -1.6382e-01\n",
      " -4.0711e-01  9.4917e-02 -2.4694e-01  4.2897e-01 -5.7419e-01 -1.3331e-01\n",
      " -6.3825e-01  6.0159e-01 -7.3611e-01  3.3953e-01  1.0755e-01 -4.4161e-01\n",
      " -5.1621e-01  9.2538e-01 -1.7751e-02  1.4353e-01  1.0766e-01 -6.3709e-01\n",
      "  1.9061e-01 -3.3838e-01  4.6853e-02  1.6434e-01 -6.8421e-01 -6.7776e-01\n",
      "  2.9103e-01 -1.2233e-01 -1.5557e-01 -1.2949e+00 -4.5583e-01 -6.3864e-01\n",
      "  4.5638e-01 -1.1198e-01  3.9628e-01  1.3045e-01  3.4738e-02 -5.7978e-01\n",
      " -5.6770e-02  3.9230e-01 -7.0704e-01  6.5727e-01  4.2612e-01  6.3776e-01\n",
      " -5.0650e-01  2.8172e-02 -1.4630e-01 -1.2322e-01  2.9535e-01 -2.9081e-01\n",
      "  4.6450e-01 -6.1105e-01 -2.4137e-01 -2.0506e-01  2.7220e-01 -1.7796e-01\n",
      "  4.9109e-01 -1.1448e-01 -3.3460e-01  2.5017e-01 -5.5843e-02  1.7946e-01\n",
      "  3.2894e-01  4.3637e-01  2.9061e-01 -2.5199e-03  7.9951e-02  1.8228e-02\n",
      "  2.1791e-01  3.9580e-01 -1.0274e-01 -1.3623e-01 -9.8154e-02 -3.5365e-01\n",
      " -2.4702e-01 -6.8414e-01  2.6346e-01 -2.8685e-01  5.5017e-01  4.3039e-01\n",
      " -3.3990e-01  4.3356e-01  7.8930e-01  6.3652e-03  1.5180e-01  6.3343e-01]\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "#Check some word vector representations\n",
    "print(glove.vector(\"decimal\"))\n",
    "print(glove.vector(\"nhibernate\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TwGFvAe59ooM"
   },
   "outputs": [],
   "source": [
    "#Define the input data for training as batches\n",
    "#For input file, the label is given by the \"stars\" column, which is the 3rd col\n",
    "#Generator of batches\n",
    "def batch_generator(train_df,batch_size,steps):\n",
    "    idx=1\n",
    "    while True: \n",
    "        yield load_data(train_df,idx-1,batch_size) # Yields data\n",
    "        if idx < steps:\n",
    "            idx+=1\n",
    "        else:\n",
    "            idx=1\n",
    "\n",
    "#Loads the requested batch given its index\n",
    "def load_data(train_df,idx,batch_size):\n",
    "    df = pd.read_csv(train_df, skiprows=idx*batch_size,nrows=batch_size)\n",
    "    x = df.iloc[:,1:]\n",
    "    y = df.iloc[:,0]\n",
    "    return (np.array(x), np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "PGP8qom0LWev"
   },
   "outputs": [],
   "source": [
    "#Sentence to sequence vectors\n",
    "def convert_to_vec(sentence, max_sequence_length, vec_dim):\n",
    "    words = sentence.split(\" \")\n",
    "    vec = np.zeros((max_sequence_length, vec_dim))\n",
    "    for i in range(max_sequence_length):\n",
    "        if i == len(words):\n",
    "            break\n",
    "        vec[i] = glove.vector(words[i])\n",
    "    return vec\n",
    "\n",
    "#Read CSV and fill up given matrix\n",
    "def read_and_parse(file_path, input_size=float(\"inf\"), max_sequence_length=30, vec_dim=300):\n",
    "    matrix = []\n",
    "    labels = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f, delimiter=\",\")\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            question = row[\"title\"]\n",
    "            vec = convert_to_vec(question, max_sequence_length, vec_dim)\n",
    "            # matrix[i,:] = vec\n",
    "            matrix.append(vec)\n",
    "            # labels[i] = float(row[\"stars\"])\n",
    "            labels.append(float(row[\"stars\"]))\n",
    "            i += 1\n",
    "            if i == input_size:\n",
    "                break\n",
    "        return np.array(matrix), np.array(labels)\n",
    "\n",
    "#Parameters\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "VEC_DIM = 300\n",
    "INPUT_SIZE_TRAIN = 97528\n",
    "INPUT_SIZE_TEST = 5418\n",
    "INPUT_SIZE_VAL = 5418\n",
    "INPUT_FILE_TRAIN = \"../processed_files/english_train.csv\"\n",
    "INPUT_FILE_TEST = \"../processed_files/english_test.csv\"\n",
    "INPUT_FILE_VAL = \"../processed_files/english_val.csv\"\n",
    "\n",
    "#Read CSV for training\n",
    "input_sequences_train,labels_train = read_and_parse(INPUT_FILE_TRAIN, max_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#Read CSV and create input matrix for testing\n",
    "input_sequences_test,labels_test = read_and_parse(INPUT_FILE_TEST, max_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#Read CSV and create input matrix for validation\n",
    "input_sequences_val,labels_val = read_and_parse(INPUT_FILE_VAL, max_sequence_length=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PasDsA6vB_Jb",
    "outputId": "a9f8fa31-edb1-47ae-fc1e-0ccb280314ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (97528, 30, 300)\n",
      "labels train: (97528,)\n",
      "input shape: (5418, 30, 300)\n",
      "labels train: (5418,)\n",
      "trues over all: 0.4639078008366828\n"
     ]
    }
   ],
   "source": [
    "print(\"input shape:\",input_sequences_train.shape)\n",
    "print(\"labels train:\",labels_train.shape)\n",
    "print(\"input shape:\",input_sequences_test.shape)\n",
    "print(\"labels train:\",labels_test.shape)\n",
    "labels_train[0]\n",
    "print(\"trues over all:\", sum(labels_train)/len(labels_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle train data\n",
    "idx = np.random.choice(range(INPUT_SIZE_TRAIN), INPUT_SIZE_TRAIN, replace=False)\n",
    "input_sequences_train = input_sequences_train[idx]\n",
    "labels_train = labels_train[idx]\n",
    "\n",
    "#Shuffle test data\n",
    "idx = np.random.choice(range(INPUT_SIZE_TEST), INPUT_SIZE_TEST, replace=False)\n",
    "input_sequences_test = input_sequences_test[idx]\n",
    "labels_test = labels_test[idx]\n",
    "\n",
    "#Shuffle val data\n",
    "idx = np.random.choice(range(INPUT_SIZE_VAL), INPUT_SIZE_VAL, replace=False)\n",
    "input_sequences_val = input_sequences_val[idx]\n",
    "labels_val = labels_val[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mGgdlYv0OSp",
    "outputId": "8a28b028-742c-46fe-a55d-a37bc3361b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30, 300)]         0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 30, 512)           1140736   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 30, 512)           1574912   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 512)               1574912   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 4,422,145\n",
      "Trainable params: 4,422,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model #1\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D, Flatten, Dense, Bidirectional, Input, Dropout\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# inputs: A 3D tensor with shape [batch, timesteps, feature].\n",
    "# inputs = tf.random.normal([32, 10, 8])\n",
    "# lstm = tf.keras.layers.LSTM(4)\n",
    "# output = lstm(inputs)\n",
    "# print(output.shape) = (32,4)\n",
    "\n",
    "input = Input(shape=(MAX_SEQUENCE_LENGTH,glove.dim))\n",
    "x = Bidirectional(LSTM(256, activation=\"relu\", return_sequences=True))(input)\n",
    "#x = Dropout(0.3)(x)\n",
    "x = Bidirectional(LSTM(256, activation=\"relu\", return_sequences=True))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Bidirectional(LSTM(256, activation=\"relu\", return_sequences=False))(x)\n",
    "x = Dropout(0.3)(x)\n",
    "#x = LSTM(256, activation=\"relu\")(x)\n",
    "x = Flatten()(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "x = Dense(256 ,activation=\"relu\")(x)\n",
    "preds = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model1 = Model(input, preds)\n",
    "model1.compile(loss='mean_squared_error',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.0003),\n",
    "              metrics=[\"accuracy\"])\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5-hAMtNFje3E",
    "outputId": "821ed2ae-5036-48ed-8879-58d858b1ce59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "762/762 [==============================] - 347s 456ms/step - loss: 0.2309 - accuracy: 0.6152 - val_loss: 0.2319 - val_accuracy: 0.6061\n",
      "Epoch 2/10\n",
      "762/762 [==============================] - 342s 448ms/step - loss: 0.2268 - accuracy: 0.6270 - val_loss: 0.2337 - val_accuracy: 0.6063\n",
      "Epoch 3/10\n",
      "762/762 [==============================] - 353s 463ms/step - loss: 0.2211 - accuracy: 0.6437 - val_loss: 0.2335 - val_accuracy: 0.6082\n",
      "Epoch 4/10\n",
      "762/762 [==============================] - 336s 442ms/step - loss: 0.2124 - accuracy: 0.6637 - val_loss: 0.2366 - val_accuracy: 0.6019\n",
      "Epoch 5/10\n",
      "762/762 [==============================] - 335s 440ms/step - loss: 0.2013 - accuracy: 0.6892 - val_loss: 0.2444 - val_accuracy: 0.6021\n",
      "Epoch 6/10\n",
      "762/762 [==============================] - 442s 581ms/step - loss: 0.1875 - accuracy: 0.7178 - val_loss: 0.2539 - val_accuracy: 0.5980\n",
      "Epoch 7/10\n",
      "762/762 [==============================] - 434s 569ms/step - loss: 0.1713 - accuracy: 0.7479 - val_loss: 0.2688 - val_accuracy: 0.5880\n",
      "Epoch 8/10\n",
      "119/762 [===>..........................] - ETA: 4:53 - loss: 0.1502 - accuracy: 0.7842"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-42662975782b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sequences_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sequences_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Fit model\n",
    "model1.fit(input_sequences_train, labels_train, validation_data=(input_sequences_test, labels_test), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZeBDT9dkmN37",
    "outputId": "1407371c-99ae-4e4a-e45b-ccc68bbcf4fa"
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "              \"compressing / decompressing folders & files\",\n",
    "              \"HOW TO decompress and compress files and folders\",\n",
    "              \"how to load a specific version of an assembly\",\n",
    "              \"how would one code test and set behavior without a special hardware instruction?\",\n",
    "             \"can you debug a .net app with only the source code of one file?\",\n",
    "             \"what columns generally make good indexes?\",\n",
    "             \"why is there no generic synchronized queue in .net?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    input_sample = convert_to_vec(question, MAX_SEQUENCE_LENGTH, VEC_DIM)\n",
    "    input_sample = input_sample[np.newaxis,...]\n",
    "    print(model1.predict(input_sample))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMInEdSQ_8xB",
    "outputId": "c66a0916-14a9-4f45-b2b4-2d215c31a02b"
   },
   "outputs": [],
   "source": [
    "predictions = model1.predict(input_sequences_test)\n",
    "y_hat_class = \n",
    "print(sum(predictions) / predictions)\n",
    "for y_hat,y  in zip(predictions[:50], labels_test[:50]):\n",
    "    print(y,\":\", y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtuCYGZBdu8t"
   },
   "outputs": [],
   "source": [
    "#create word to index dictionary: word->index\n",
    "# index 0 is for unnexistent word\n",
    "word_index = {}\n",
    "cont = 0\n",
    "for word in glove.embeddings.keys():\n",
    "    cont +=1\n",
    "    word_index[word] = cont\n",
    "\n",
    "#Processing of GloVe data into the embedding matrix to use in Keras\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, glove.dim))\n",
    "EMBEDDING_DIM = glove.dim\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_matrix[i] = glove.vector(word)\n",
    "\n",
    "# delete glove dictionary to save memory RAM\n",
    "#del glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zS7JR6Rix9UU"
   },
   "outputs": [],
   "source": [
    "# Create model #1\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "#del embedding_matrix\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(16, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(16, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(16, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(1, activation='relu')(x)\n",
    "\n",
    "model1 = Model(sequence_input, preds)\n",
    "model1.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "# model1.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "#           epochs=2, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9UrFm5xyGBz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
